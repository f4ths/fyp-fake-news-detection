{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "import spacy\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout,Bidirectional\n",
    "from keras.utils import pad_sequences\n",
    "from keras_preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data from csv to pandas dataframe\n",
    "# train_data = pd.read_csv('processed_train_data.tsv', sep='\\t', index_col=0)\n",
    "# val_data = pd.read_csv('processed_val_data.tsv', sep='\\t', index_col=0)\n",
    "# test_data = pd.read_csv('processed_test_data.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# APR 5 TODO\n",
    "    - [ ] finalise experiments set up\n",
    "    - [ ] run experiments\n",
    "    - [ ] go back to preprocessing to add dep parse\n",
    "    - [ ] include dep in experiments set up\n",
    "    - [ ] 2nd run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Alternative way to load data - pickle to pandas dataframe\n",
    "train_data = pd.read_pickle('processed_train_data.p')\n",
    "val_data = pd.read_pickle('processed_val_data.p')\n",
    "test_data = pd.read_pickle('processed_test_data.p')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def format_string2np(string_numpy):\n",
    "    \"\"\"\n",
    "    Converts string representation of a numpy array to a numpy array.\n",
    "    Necessary because pandas dataframe cannot store numpy arrays.\n",
    "\n",
    "    :param string_numpy: {str} string representation of an array\n",
    "    :return: numpy array\n",
    "    \"\"\"\n",
    "    \"\"\"formatting : Conversion of String List to List\n",
    "\n",
    "    Args:\n",
    "        string_numpy (str)\n",
    "    Returns:\n",
    "        l (list): list of values\n",
    "    \"\"\"\n",
    "    list_values = string_numpy.strip('[]').split(', ')\n",
    "    return np.array(list_values).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# reformat dataframe columns from string to numpy arrays\n",
    "columns = ['word_id_custom', 'word_id_spacy', 'pos_id', 'pos_id_DEFAULT']\n",
    "\n",
    "for col in columns:\n",
    "    train_data[col] = train_data[col].apply(format_string2np)\n",
    "    val_data[col] = val_data[col].apply(format_string2np)\n",
    "    test_data[col] = test_data[col].apply(format_string2np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load POS dictionaries\n",
    "with open('pos_dicts.pickle', 'rb') as f:\n",
    "    pos_dict_custom, pos_dict_default = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrixes: (9607, 100) (9496, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding matrixes\n",
    "embedding_matrix_custom_100d = np.load('embedding_matrix_custom_100d.npy')\n",
    "embedding_matrix_spacy_100d = np.load('embedding_matrix_spacy_100d.npy')\n",
    "embedding_matrix_custom_300d = np.load('embedding_matrix_custom_300d.npy')\n",
    "embedding_matrix_spacy_300d = np.load('embedding_matrix_spacy_300d.npy')\n",
    "\n",
    "print(\"Shape of the embedding matrixes:\", embedding_matrix_custom_100d.shape, embedding_matrix_spacy_300d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary dictionary: 9606\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary dictionaries\n",
    "vocabulary_dict_custom = pickle.load(open('vocabulary_statement_custom.p', 'rb'))\n",
    "vocabulary_dict_spacy = pickle.load(open('vocabulary_statement_spacy.p', 'rb'))\n",
    "vocab_length = len(vocabulary_dict_custom)\n",
    "\n",
    "print(\"Length of the vocabulary dictionary:\", vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   index          id        label  \\\n0      0   2635.json        false   \n1      1  10540.json    half-true   \n2      2    324.json  mostly-true   \n3      3   1123.json        false   \n4      4   9028.json    half-true   \n\n                                           statement  \\\n0  Says the Annies List political group supports ...   \n1  When did the decline of coal start? It started...   \n2  Hillary Clinton agrees with John McCain \"by vo...   \n3  Health care reform legislation is likely to ma...   \n4  The economic turnaround started at the end of ...   \n\n                              subject         speaker             job_title  \\\n0                            abortion    dwayne-bohac  State representative   \n1  energy,history,job-accomplishments  scott-surovell        State delegate   \n2                      foreign-policy    barack-obama             President   \n3                         health-care    blog-posting                         \n4                        economy,jobs   charlie-crist                         \n\n  state_info       party  barely true  ...  job_id  state_id  party_id  \\\n0      Texas  republican          0.0  ...       1         1         0   \n1   Virginia    democrat          0.0  ...       6         7         1   \n2   Illinois    democrat         70.0  ...       2         3         1   \n3                   none          7.0  ...       0         0         2   \n4    Florida    democrat         15.0  ...       0         2         1   \n\n   context_id                                             pos_id  \\\n0           2      [1, 14, 8, 8, 2, 0, 1, 2, 10, 0, 0, 4, 0, 10]   \n1           2  [7, 1, 14, 0, 4, 0, 0, 10, 5, 1, 7, 2, 0, 1, 4...   \n2           8  [8, 8, 1, 4, 8, 8, 10, 4, 1, 12, 1, 8, 8, 14, ...   \n3           0         [0, 0, 0, 0, 11, 2, 12, 1, 2, 0, 0, 0, 10]   \n4           1               [14, 2, 0, 1, 4, 14, 0, 4, 5, 0, 10]   \n\n                                      pos_id_DEFAULT  \\\n0   [16, 5, 11, 11, 0, 7, 16, 0, 12, 7, 7, 1, 7, 12]   \n1  [14, 16, 5, 7, 1, 7, 7, 12, 10, 16, 14, 0, 7, ...   \n2  [11, 11, 16, 1, 11, 11, 12, 1, 16, 9, 16, 11, ...   \n3          [7, 7, 7, 7, 3, 0, 9, 16, 0, 7, 7, 7, 12]   \n4               [5, 0, 7, 16, 1, 5, 7, 1, 10, 7, 12]   \n\n                                    statement_custom  \\\n0  say annies list political group support third ...   \n1  when do decline coal start start when natural ...   \n2  hillary clinton agree john mccain vote give ge...   \n3  health care reform legislation be likely manda...   \n4                 economic turnaround start end term   \n\n                                     statement_spacy  \\\n0  say annies list political group support trimes...   \n1  decline coal start start natural gas take star...   \n2  hillary clinton agree john mccain vote george ...   \n3  health care reform legislation likely mandate ...   \n4                 economic turnaround start end term   \n\n                                      word_id_custom  \\\n0  [3, 5440, 717, 493, 396, 54, 274, 4039, 155, 1...   \n1  [37, 9, 804, 861, 308, 308, 37, 981, 254, 39, ...   \n2  [104, 69, 734, 160, 201, 18, 89, 262, 137, 258...   \n3  [19, 22, 209, 252, 1, 592, 406, 361, 439, 176,...   \n4                         [282, 3331, 308, 247, 248]   \n\n                                       word_id_spacy  \n0       [1, 5315, 633, 423, 332, 37, 3919, 120, 936]  \n1  [720, 773, 249, 249, 891, 204, 46, 249, 527, 1...  \n2  [74, 49, 649, 125, 157, 12, 212, 103, 208, 274...  \n3  [13, 16, 165, 202, 514, 342, 301, 372, 140, 2747]  \n4                         [224, 3208, 249, 198, 199]  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>label</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>job_title</th>\n      <th>state_info</th>\n      <th>party</th>\n      <th>barely true</th>\n      <th>...</th>\n      <th>job_id</th>\n      <th>state_id</th>\n      <th>party_id</th>\n      <th>context_id</th>\n      <th>pos_id</th>\n      <th>pos_id_DEFAULT</th>\n      <th>statement_custom</th>\n      <th>statement_spacy</th>\n      <th>word_id_custom</th>\n      <th>word_id_spacy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2635.json</td>\n      <td>false</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[1, 14, 8, 8, 2, 0, 1, 2, 10, 0, 0, 4, 0, 10]</td>\n      <td>[16, 5, 11, 11, 0, 7, 16, 0, 12, 7, 7, 1, 7, 12]</td>\n      <td>say annies list political group support third ...</td>\n      <td>say annies list political group support trimes...</td>\n      <td>[3, 5440, 717, 493, 396, 54, 274, 4039, 155, 1...</td>\n      <td>[1, 5315, 633, 423, 332, 37, 3919, 120, 936]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10540.json</td>\n      <td>half-true</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>Virginia</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[7, 1, 14, 0, 4, 0, 0, 10, 5, 1, 7, 2, 0, 1, 4...</td>\n      <td>[14, 16, 5, 7, 1, 7, 7, 12, 10, 16, 14, 0, 7, ...</td>\n      <td>when do decline coal start start when natural ...</td>\n      <td>decline coal start start natural gas take star...</td>\n      <td>[37, 9, 804, 861, 308, 308, 37, 981, 254, 39, ...</td>\n      <td>[720, 773, 249, 249, 891, 204, 46, 249, 527, 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>324.json</td>\n      <td>mostly-true</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>8</td>\n      <td>[8, 8, 1, 4, 8, 8, 10, 4, 1, 12, 1, 8, 8, 14, ...</td>\n      <td>[11, 11, 16, 1, 11, 11, 12, 1, 16, 9, 16, 11, ...</td>\n      <td>hillary clinton agree john mccain vote give ge...</td>\n      <td>hillary clinton agree john mccain vote george ...</td>\n      <td>[104, 69, 734, 160, 201, 18, 89, 262, 137, 258...</td>\n      <td>[74, 49, 649, 125, 157, 12, 212, 103, 208, 274...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1123.json</td>\n      <td>false</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td></td>\n      <td></td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>[0, 0, 0, 0, 11, 2, 12, 1, 2, 0, 0, 0, 10]</td>\n      <td>[7, 7, 7, 7, 3, 0, 9, 16, 0, 7, 7, 7, 12]</td>\n      <td>health care reform legislation be likely manda...</td>\n      <td>health care reform legislation likely mandate ...</td>\n      <td>[19, 22, 209, 252, 1, 592, 406, 361, 439, 176,...</td>\n      <td>[13, 16, 165, 202, 514, 342, 301, 372, 140, 2747]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9028.json</td>\n      <td>half-true</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td></td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[14, 2, 0, 1, 4, 14, 0, 4, 5, 0, 10]</td>\n      <td>[5, 0, 7, 16, 1, 5, 7, 1, 10, 7, 12]</td>\n      <td>economic turnaround start end term</td>\n      <td>economic turnaround start end term</td>\n      <td>[282, 3331, 308, 247, 248]</td>\n      <td>[224, 3208, 249, 198, 199]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## to test the different preprocessing methods, the following experiments will be run:\n",
    "1. siddarth - baseline\n",
    "\n",
    "        - ogdict\n",
    "        - nltk stopword removal only\n",
    "\n",
    "****\n",
    "****\n",
    "\n",
    "## todo\n",
    "****\n",
    "### INITIALISE INPUT/OUTPUT\n",
    "1. [x] word_id custom AS X_train_custom\n",
    "  - [x] embedding_matrix_custom\n",
    "  - [x] vocabulary_dict_custom\n",
    "2. [x] word_id spacy AS X_train_spacy\n",
    "  - [x] embedding_matrix_spacy\n",
    "  - [x] vocabulary_dict_spacy\n",
    "  \n",
    "3. [x] pos_id custom AS X_train_pos_custom\n",
    "4. [x] pos_id spacy AS X_train_pos_spacy\n",
    "    \n",
    "#### Input variables to be processed: \n",
    "- [ ] meta\n",
    "- [ ] dep parse\n",
    "\n",
    "****\n",
    "### GENERAL\n",
    "- [x] change everything to python 3\n",
    "\n",
    "### variables, init, etc.\n",
    "- [ ] pass in vocabulary.p\n",
    "****\n",
    "### BILSTM MODEL\n",
    "- [ ] functions: train(), etc\n",
    "        (CODE CELLS COULD BE BETTER, INVESTIGATE)\n",
    "- [ ] black box everything to understand la\n",
    "- [ ]\n",
    "\n",
    "****\n",
    "*decide which word id to use*\n",
    "\n",
    "1. fathan - pos tag check [ ]\n",
    "    - custom pos tag dict\n",
    "    - spacy preprocess - spacy word id\n",
    "    - glove\n",
    " 2. fathan - preprocess [ ]\n",
    "    - custom pos tag dict\n",
    "    - custom preprocess - custom word id\n",
    "    - glove\n",
    "    * check best results with default pos tag [ ]\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# HYPERPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "30\n",
      "18\n",
      "19\n",
      "25\n",
      "26\n",
      "Index(['index', 'id', 'label', 'statement', 'subject', 'speaker', 'job_title',\n",
      "       'state_info', 'party', 'barely true', 'false', 'half-true',\n",
      "       'mostly-true', 'pants-on-fire', 'context', 'output', 'subject_id',\n",
      "       'speaker_id', 'job_id', 'state_id', 'party_id', 'context_id', 'pos_id',\n",
      "       'pos_id_DEFAULT', 'statement_custom', 'statement_spacy',\n",
      "       'word_id_custom', 'word_id_spacy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 100d GLOVE\n",
    "EMBED_DIM = 100\n",
    "\n",
    "# vocab_length = len(vocabulary_dict.keys())\n",
    "custom_vocabLen = len(vocabulary_dict_custom.keys())\n",
    "spacy_vocabLen = len(vocabulary_dict_spacy.keys())\n",
    "hidden_size = EMBED_DIM #Has to be same as EMBED_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 15\n",
    "num_epochs = 30\n",
    "batch_size = 40\n",
    "\n",
    "embedding_matrix = embedding_matrix_custom_100d\n",
    "\n",
    "## TODO: preprocess metadata\n",
    "#Meta data related hyper params\n",
    "num_party = len(train_data['party_id'].unique()) # APRIL 2 todo 1 - process party id in preprocessing.ipynb\n",
    "num_state = len(train_data['state_id'].unique())\n",
    "num_context = len(train_data['context_id'].unique())\n",
    "num_job = len(train_data['job_id'].unique())\n",
    "num_sub = len(train_data['subject_id'].unique())\n",
    "num_speaker = len(train_data['speaker_id'].unique())\n",
    "\n",
    "print(num_party)\n",
    "print(num_state)\n",
    "print(num_context)\n",
    "print(num_job)\n",
    "print(num_sub)\n",
    "print(num_speaker)\n",
    "print(train_data.columns)\n",
    "\n",
    "# print num_party\n",
    "# print num_state\n",
    "# print num_venue\n",
    "# print num_job\n",
    "# print num_sub\n",
    "# print num_speaker\n",
    "# print train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [3, 5440, 717, 493, 396, 54, 274, 4039, 155, 1...\n",
      "1        [37, 9, 804, 861, 308, 308, 37, 981, 254, 39, ...\n",
      "2        [104, 69, 734, 160, 201, 18, 89, 262, 137, 258...\n",
      "3        [19, 22, 209, 252, 1, 592, 406, 361, 439, 176,...\n",
      "4                               [282, 3331, 308, 247, 248]\n",
      "                               ...                        \n",
      "10235    [1, 161, 133, 5206, 314, 84, 86, 1, 408, 212, ...\n",
      "10236       [157, 2, 52, 283, 213, 464, 1399, 653, 4, 389]\n",
      "10237    [3, 2024, 154, 110, 1205, 3983, 124, 38, 2, 42...\n",
      "10238       [1604, 28, 13, 2857, 3115, 4, 153, 1391, 1473]\n",
      "10239    [216, 1429, 2087, 2, 5142, 826, 494, 327, 441,...\n",
      "Name: word_id_custom, Length: 10240, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_data['word_id_custom'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input/Output Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def input_pad_sequences(data):\n",
    "    padded_data = sequence.pad_sequences(data, maxlen=num_steps, padding='post', truncating='post')\n",
    "    return padded_data\n",
    "\n",
    "X_train_custom = input_pad_sequences(train_data['word_id_custom'])\n",
    "X_val_custom = input_pad_sequences(val_data['word_id_custom'])\n",
    "X_test_custom = input_pad_sequences(test_data['word_id_custom'])\n",
    "\n",
    "X_train_spacy = input_pad_sequences(train_data['word_id_spacy'])\n",
    "X_val_spacy = input_pad_sequences(val_data['word_id_spacy'])\n",
    "X_test_spacy = input_pad_sequences(test_data['word_id_spacy'])\n",
    "\n",
    "X_train_pos = input_pad_sequences(train_data['pos_id'])\n",
    "X_val_pos = input_pad_sequences(val_data['pos_id'])\n",
    "X_test_pos = input_pad_sequences(test_data['pos_id'])\n",
    "\n",
    "X_train_pos_DEFAULT = input_pad_sequences(train_data['pos_id_DEFAULT'])\n",
    "X_val_pos_DEFAULT = input_pad_sequences(val_data['pos_id_DEFAULT'])\n",
    "X_test_pos_DEFAULT = input_pad_sequences(test_data['pos_id_DEFAULT'])\n",
    "\n",
    "#TODO: preprocess dependency parse\n",
    "# X_train_dep = input_pad_sequences(train_data['dep_id'])\n",
    "# X_val_dep = input_pad_sequences(val_data['dep_id'])\n",
    "# X_test_dep = input_pad_sequences(test_data['dep_id'])\n",
    "\n",
    "#Meta data preparation\n",
    "party_train = keras.utils.to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "state_train = keras.utils.to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "context_train = keras.utils.to_categorical(train_data['context_id'], num_classes=num_context)\n",
    "job_train = keras.utils.to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "subject_train = keras.utils.to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "speaker_train = keras.utils.to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "#X_train_meta = party_train\n",
    "X_train_meta = np.hstack((party_train, state_train, context_train, job_train, subject_train, speaker_train))\n",
    "\n",
    "party_val = keras.utils.to_categorical(val_data['party_id'], num_classes=num_party)\n",
    "state_val = keras.utils.to_categorical(val_data['state_id'], num_classes=num_state)\n",
    "context_val = keras.utils.to_categorical(val_data['context_id'], num_classes=num_context)\n",
    "job_val = keras.utils.to_categorical(val_data['job_id'], num_classes=num_job)\n",
    "subject_val = keras.utils.to_categorical(val_data['subject_id'], num_classes=num_sub)\n",
    "speaker_val = keras.utils.to_categorical(val_data['speaker_id'], num_classes=num_speaker)\n",
    "#X_val_meta = party_val\n",
    "X_val_meta = np.hstack((party_val, state_val, context_val, job_val, subject_val, speaker_val))\n",
    "\n",
    "party_test = keras.utils.to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "state_test = keras.utils.to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "context_test = keras.utils.to_categorical(test_data['context_id'], num_classes=num_context)\n",
    "job_test = keras.utils.to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "subject_test = keras.utils.to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "speaker_test = keras.utils.to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "#X_test_meta = party_test\n",
    "X_test_meta = np.hstack((party_test, state_test, context_test, job_test, subject_test, speaker_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3 5440  717 ...    0    0    0]\n",
      " [  37    9  804 ...   16  262  517]\n",
      " [ 104   69  734 ...    0    0    0]\n",
      " ...\n",
      " [   3 2024  154 ...   24 1311 1173]\n",
      " [1604   28   13 ...    0    0    0]\n",
      " [ 216 1429 2087 ... 1195  430  184]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_custom)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Initialise input/output\n",
    "\n",
    "## OUTPUT\n",
    "Y_train = train_data['output']\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=6)\n",
    "\n",
    "Y_val = val_data['output']\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 350  425   28 ...    0    0    0]\n",
      " [  64    1 1930 ...    0    0    0]\n",
      " [   3  160  201 ...    0    0    0]\n",
      " ...\n",
      " [ 472 1547  191 ...  142  524  372]\n",
      " [   3 1406  928 ...   78    0    0]\n",
      " [   3   93    1 ...  258   43  107]]\n",
      "[[ 291  358   19 ...    0    0    0]\n",
      " [  45 1829  234 ...    0    0    0]\n",
      " [   1  125  157 ...    0    0    0]\n",
      " ...\n",
      " [ 405 1448  153 ...  106  451  310]\n",
      " [   1 1306  836 ...    0    0    0]\n",
      " [   1   64   34 ... 1571   89   77]]\n",
      "[[ 1 14  0 ...  0  0  0]\n",
      " [ 8 11  4 ... 10  0  0]\n",
      " [ 1  8  8 ...  0  0  0]\n",
      " ...\n",
      " [ 4 14  2 ...  2  0  1]\n",
      " [ 1 14  8 ...  1  5  1]\n",
      " [ 1 14  0 ... 14  5 11]]\n",
      "[[16  5  7 ...  0  0  0]\n",
      " [11  3  1 ... 12  0  0]\n",
      " [16 11 11 ...  0  0  0]\n",
      " ...\n",
      " [ 1  5  0 ...  0  7 16]\n",
      " [16  5 11 ... 16 10 16]\n",
      " [16  5  7 ... 13 10  3]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_custom)\n",
    "print(X_test_spacy)\n",
    "print(X_test_pos)\n",
    "print(X_test_pos_DEFAULT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 15) (1284, 15) (1267, 15)\n",
      "(10240, 6) (1284, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_custom.shape, X_val_custom.shape, X_test_custom.shape)\n",
    "print(X_train_spacy.shape, X_val_spacy.shape, X_test_spacy.shape)\n",
    "print(X_train_pos.shape, X_val_pos.shape, X_test_pos.shape)\n",
    "print(X_train_pos_DEFAULT.shape, X_val_pos_DEFAULT.shape, X_test_pos_DEFAULT.shape)\n",
    "print(Y_train.shape, Y_val.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. fathan - pos tag check [ ]\n",
    "    - custom word id [ ]\n",
    "    - custom pos id [ ]\n",
    "    - glove [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# def train(model, name, *args, **kwargs):\n",
    "#     # sgd = optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "#     # adam = optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#     # model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#     # tb = TensorBoard()\n",
    "#     # csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "#     # filepath = name + \"_weights_best.hdf5\"\n",
    "#     # checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy',\n",
    "#     #                                               verbose=1, save_best_only=True, mode='max')\n",
    "#     # inputs = {}\n",
    "#     # for arg in args:\n",
    "#     #     inputs.update(arg)\n",
    "#     # inputs.update(kwargs)\n",
    "#     #\n",
    "#     # validation_inputs = {}\n",
    "#     # for arg in args:\n",
    "#     #     validation_inputs.update(arg)\n",
    "#     # validation_inputs.update(kwargs)\n",
    "#     #\n",
    "#     # model.fit(\n",
    "#     #     inputs,\n",
    "#     #     {'main_output': Y_train},\n",
    "#     #     epochs=num_epochs,\n",
    "#     #     batch_size=batch_size,\n",
    "#     #     validation_data=(\n",
    "#     #         validation_inputs,\n",
    "#     #         {'main_output': Y_val}\n",
    "#     #     ),\n",
    "#     #     callbacks=[tb, csv_logger, checkpoint]\n",
    "#     # )\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    Compiles the given model with SGD optimizer, categorical_crossentropy loss, and categorical_accuracy metrics.\n",
    "\n",
    "    :param model: {tf.keras.Model} the model to be compiled\n",
    "    \"\"\"\n",
    "    sgd = optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "def create_callbacks(name):\n",
    "    \"\"\"\n",
    "    Creates a list of callbacks for model training.\n",
    "\n",
    "    :param name: {str} model name used for saving best weights\n",
    "\n",
    "    :return: {list} list of callbacks\n",
    "    \"\"\"\n",
    "    tb = TensorBoard()\n",
    "    csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "    filepath = name + \"_weights_best.hdf5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy',\n",
    "                                                  verbose=1, save_best_only=True, mode='max')\n",
    "    return [tb, csv_logger, checkpoint]\n",
    "\n",
    "def merge_dicts(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Merges the input dictionaries from args and kwargs.\n",
    "\n",
    "    :param *args: {tuple} variable length argument list for input dictionaries\n",
    "    :param **kwargs: {dict} arbitrary keyword arguments for input dictionaries\n",
    "\n",
    "    :return: {dict} merged dictionary\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for arg in args:\n",
    "        inputs.update(arg)\n",
    "    inputs.update(kwargs)\n",
    "    return inputs\n",
    "\n",
    "def train(model, name, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains the given model with the provided input arguments and keyword arguments.\n",
    "\n",
    "    :param model: {tf.keras.Model} the model to be trained\n",
    "    :param name: {str} model name used for saving best weights\n",
    "    :param *args: {tuple} variable length argument list for input dictionaries\n",
    "    :param **kwargs: {dict} arbitrary keyword arguments for input dictionaries\n",
    "    \"\"\"\n",
    "    compile_model(model)\n",
    "    callbacks = create_callbacks(name)\n",
    "    inputs = merge_dicts(*args, **kwargs)\n",
    "    validation_inputs = merge_dicts(*args, **kwargs)\n",
    "\n",
    "    model.fit(\n",
    "        inputs,\n",
    "        {'main_output': Y_train},\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            validation_inputs,\n",
    "            {'main_output': Y_val}\n",
    "        ),\n",
    "        callbacks=callbacks\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def load_and_predict(name, inputs):\n",
    "    \"\"\"\n",
    "    Load the model and predict with the provided inputs\n",
    "    :param name: {str} name of the model to load the model weights\n",
    "    :param inputs: {dict} dictionary of inputs to be passed to the model\n",
    "\n",
    "    :return: preds {np.array} array of predicted values/\n",
    "    \"\"\"\n",
    "    model = load_model(name + \"_weights_best.hdf5\")\n",
    "    preds = model.predict(inputs,\n",
    "                          batch_size=batch_size,\n",
    "                          verbose=1)\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, Y_test_gt):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the predictions\n",
    "\n",
    "    :param predictions: {np.array} array of predicted values\n",
    "    :param Y_test_groundtruth: {np.array} array of ground truth values\n",
    "\n",
    "    :return: accuracy {float} accuracy of the predictions\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == Y_test_gt[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(predictions)\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def extract_fw_tb(preds):\n",
    "    \"\"\"\n",
    "    Extracts the worst false predictions and the best true predictions\n",
    "\n",
    "    :param preds: {np.array} array of predicted values\n",
    "\n",
    "    :return: false_worst: {dict} dictionary of worst false predictions\n",
    "    :return: true_best: {dict} dictionary of best true predictions\n",
    "    \"\"\"\n",
    "    false_worst = {}\n",
    "    true_best = {}\n",
    "\n",
    "    for p in range(len(preds)):\n",
    "        if np.argmax(preds[p])==0:\n",
    "            false_worst[p]=preds[p][0]\n",
    "        elif np.argmax(preds[p])==5:\n",
    "            true_best[p]=preds[p][5]\n",
    "\n",
    "    return false_worst, true_best"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def evaluate(model_name, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates the given model with provided input configurations\n",
    "\n",
    "    :param model_name: {str} used to load model weights\n",
    "    :param args: {list} of dictionaries of inputs\n",
    "    :param kwargs:\n",
    "\n",
    "    :return: false_worst: {dict} of the worst false predictions\n",
    "    :return: true_best: {dict} of the best true predictions\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for arg in args:\n",
    "        inputs.update(arg)\n",
    "    inputs.update(kwargs)\n",
    "\n",
    "    preds = load_and_predict(model_name, inputs)\n",
    "\n",
    "    Y_test_groundtruth = list(test_data['output'])\n",
    "    predictions = np.array([np.argmax(pred) for pred in preds])\n",
    "\n",
    "    accuracy = calculate_accuracy(predictions, Y_test_groundtruth)\n",
    "    print(\"Correctly Predicted: \", np.sum(predictions == Y_test_groundtruth), \"/\", len(Y_test_groundtruth))\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    false_worst, true_best = extract_fw_tb(preds)\n",
    "\n",
    "    pickle.dump(predictions, open(model_name + \"_predictions.p\", \"wb\"))\n",
    "    return false_worst, true_best\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def evaluate(name, *args, **kwargs):\n",
    "#     model = load_model(name + \"_weights_best.hdf5\")\n",
    "#     inputs = {}\n",
    "#     for arg in args:\n",
    "#         inputs.update(arg)\n",
    "#     inputs.update(kwargs)\n",
    "#\n",
    "#     preds = model.predict(inputs,\n",
    "#                           batch_size=batch_size,\n",
    "#                           verbose=1)\n",
    "#\n",
    "#     false_worst = {}\n",
    "#     true_best = {}\n",
    "#\n",
    "#     Y_test_gt = list(test_data['output'])\n",
    "#     predictions = np.array([np.argmax(pred) for pred in preds])\n",
    "#\n",
    "#     for p in range(len(preds)):\n",
    "#         if np.argmax(preds[p])==0:\n",
    "#             false_worst[p]=preds[p][0]\n",
    "#         elif np.argmax(preds[p])==5:\n",
    "#             true_best[p]=preds[p][5]\n",
    "#\n",
    "#     # print(len(predictions))==len(Y_test_gt)\n",
    "#     correct = np.sum(predictions == Y_test_gt)\n",
    "#     print(\"Correctly Predicted: \", correct,\"/\",len(Y_test_gt))\n",
    "#     print(\"Accuracy: \", correct*100.0/len(Y_test_gt))\n",
    "#\n",
    "#     pickle.dump(predictions, open(name+\"_predictions.p\", \"wb\"))\n",
    "#     return false_worst, true_best"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "configurations = [\n",
    "\t{\n",
    "        \"name\": \"fathan_1\",\n",
    "        \"inputs\": [X_test_custom, X_test_pos],\n",
    "    },\n",
    "\t{\n",
    "        \"name\": \"fathan_2\",\n",
    "        \"inputs\": [X_test_spacy, X_test_pos],\n",
    "    },\n",
    "\t{\n",
    "        \"name\": \"fathan_3\",\n",
    "        \"inputs\": [X_test_custom, X_test_pos_DEFAULT],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fathan_4\",\n",
    "        \"inputs\": [X_test_custom, X_test_pos_DEFAULT],\n",
    "    },\n",
    "\t# {\n",
    "    #     \"name\": \"fathan_5\",\n",
    "    #     \"inputs\": [X_test_best, X_test_pos_best, X_test_meta],\n",
    "    # },\n",
    "\t# {\n",
    "    #     \"name\": \"fathan_6\",\n",
    "    #     \"inputs\": [X_test_best, X_test_pos_best, X_test_meta, X_test_dep],\n",
    "    # },\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_name = \"bilstm\"\n",
    "#\n",
    "# for config in configurations:\n",
    "#     print(f\"Running {config['name']}...\")\n",
    "#     (fw, tb) = evaluate(model_name, *config[\"inputs\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(fw, tb) = evaluate('bilstm',\n",
    "                    X_test_custom,\n",
    "                    X_test_pos,\n",
    "                    # X_test_dep,\n",
    "                    # X_test_meta\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def print_best_false_true_predicted(fw, tb):\n",
    "  sorted_false = sorted(fw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  sorted_true = sorted(tb.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  print('*****************************************************************')\n",
    "  print('******************** False statements *************************')\n",
    "\n",
    "  for t in sorted_false[:5]:\n",
    "    print(t[1])\n",
    "    print(test_data.loc[t[0]])\n",
    "    print('=============')\n",
    "  print('*****************************************************************')\n",
    "  print('******************** True Statements *************************')\n",
    "  for t in sorted_true[:5]:\n",
    "    print(t[1])\n",
    "    print(test_data.loc[t[0]])\n",
    "    print('=============')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BILSTM model\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
    "model_bilstm.add(Bidirectional(LSTM(hidden_size)))\n",
    "model_bilstm.add(Dense(6, activation='softmax'))\n",
    "\n",
    "pos_dict = pos_dict_custom\n",
    "pos_embeddings = np.identity(max(pos_dict.values()), dtype=int)\n",
    "\n",
    "# statement embed biLSTM\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x = Embedding(vocab_length+1,EMBED_DIM,weights=[embedding_matrix],input_length=num_steps,trainable=False)(statement_input)\n",
    "bilstm_word_input = LSTM(lstm_size, dropout=0.2)(x)\n",
    "\n",
    "# pos embed biLSTM\n",
    "pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "x2 = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
    "bilstm_pos_in = LSTM(lstm_size, dropout=0.2)(x2)\n",
    "\n",
    "\n",
    "# todo: DEP AND META\n",
    "\n",
    "# # dep embed LSTM X3\n",
    "# dep_input = Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "# x3 = Embedding(max(dep_dict.values()), max(dep_dict.values()), weights=[dep_embeddings], input_length=num_steps, trainable=False)(dep_input)\n",
    "# lstm_in3 = LSTM(lstm_size, dropout=0.2)(x3)\n",
    "\n",
    "# meta data Dense\n",
    "meta_input = Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = Dense(64, activation='relu')(meta_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def concat_lstm_layers(bilstm_input, layers_to_concat):\n",
    "    layers = [bilstm_input] + layers_to_concat\n",
    "    x = keras.layers.concatenate(layers)\n",
    "    return x\n",
    "\n",
    "extra_layers = [bilstm_pos_in] #, lstm_in3, x_meta]\n",
    "# exp1_layers = [bilstm_pos_in, lstm_in3, x_meta]\n",
    "# exp2_layers = [bilstm_pos_in, x_meta]\n",
    "x = concat_lstm_layers(bilstm_word_input, extra_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main_output = Dense(6, activation='softmax', name='main_output')(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_lstm = Model(inputs=[statement_input, pos_input, dep_input, meta_input], outputs=[main_output])\n",
    "# model_lstm = Model(inputs=[statement_input, pos_input, meta_input], outputs=[main_output])\n",
    "# model_lstm = Model(inputs=[statement_input, dep_input, meta_input], outputs=[main_output])\n",
    "# model_lstm = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "experiment1input = [statement_input, pos_input, dep_input, meta_input]\n",
    "experiment2input = [statement_input, pos_input, meta_input]\n",
    "experiment3input = [statement_input, dep_input, meta_input]\n",
    "\n",
    "model_bilstm = Model(inputs=experiment3input, outputs=[main_output])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model_bilstm,\n",
    "      'bilstm',\n",
    "      {'main_input': X_train_custom, 'pos_input': X_train_pos},\n",
    "      # {'aux_input': X_train_meta},\n",
    "      validation_data=(\n",
    "\t      {'main_input': X_val_custom, 'pos_input': X_val_pos},\n",
    "\t      # {'aux_input': X_val_meta}\n",
    "      )\n",
    "      )\n",
    "train(model_bilstm,\n",
    "      'bilstm',\n",
    "      {'main_input': X_train_spacy, 'pos_input': X_train_pos},\n",
    "      # {'aux_input': X_train_meta},\n",
    "      validation_data=(\n",
    "\t      {'main_input': X_val_spacy, 'pos_input': X_val_pos},\n",
    "\t      # {'aux_input': X_val_meta}\n",
    "      )\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def train(model, name, *args, use_pos=False, use_meta=False, use_dep=False, **kwargs):\n",
    "#     sgd = optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "#     adam = optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#     tb = TensorBoard()\n",
    "#     csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "#     filepath = name + \"_weights_best.hdf5\"\n",
    "#     checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy',\n",
    "#                                                   verbose=1, save_best_only=True, mode='max')\n",
    "#\n",
    "#     inputs = {}\n",
    "#     for arg in args:\n",
    "#         inputs.update(arg)\n",
    "#\n",
    "#     inputs.update(kwargs)\n",
    "#\n",
    "#     model.fit(\n",
    "#         inputs,\n",
    "#         {'main_output': Y_train}, epochs=num_epochs, batch_size=batch_size,\n",
    "#         validation_data=(\n",
    "#             inputs,\n",
    "#             {'main_output': Y_val}\n",
    "#         ), callbacks=[tb, csv_logger, checkpoint]\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train(model, name, {'main_input': X_train, 'pos_input': X_train_pos}, {'aux_input': X_train_meta})\n",
    "# train(model, name, main_input=X_train, pos_input=X_train_pos, aux_input=X_train_meta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def train(model, name, use_pos=False, use_meta=False, use_dep=False):\n",
    "#     sgd = optimizers.SGD(lr=0.025, clipvalue=0.3, nesterov=True)\n",
    "#     adam = optimizers.Adam(lr=0.000075, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#     tb = TensorBoard()\n",
    "#     csv_logger = ker\n",
    "#     as.callbacks.CSVLogger('training.log')\n",
    "#     filepath = name + \"_weights_best.hdf5\"\n",
    "#     checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#\n",
    "#     if use_pos and use_meta:\n",
    "#         if use_dep:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'pos_input': X_train_pos, 'aux_input': X_train_meta, 'dep_input': X_train_dep},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'pos_input': X_val_pos, 'aux_input': X_val_meta, 'dep_input': X_val_dep},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#         else:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'pos_input': X_train_pos, 'aux_input': X_train_meta},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'pos_input': X_val_pos, 'aux_input': X_val_meta},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#     elif use_meta:\n",
    "#         if use_dep:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'aux_input': X_train_meta, 'dep_input': X_train_dep},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'aux_input': X_val_meta, 'dep_input': X_val_dep},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#         else:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'aux_input': X_train_meta},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'aux_input': X_val_meta},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#     elif use_pos:\n",
    "#         if use_dep:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'pos_input': X_train_pos, 'dep_input': X_train_dep},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'pos_input': X_val_pos, 'dep_input': X_val_dep},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#         else:\n",
    "#             model.fit(\n",
    "#                 {'main_input': X_train, 'pos_input': X_train_pos},\n",
    "#                 {'main_output': Y_train},\n",
    "#                 epochs=num_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(\n",
    "#                     {'main_input': X_val, 'pos_input': X_val_pos},\n",
    "#                     {'main_output': Y_val}),\n",
    "#                 callbacks=[tb, csv_logger, checkpoint])\n",
    "#     else:\n",
    "#         if use_dep:\n",
    "#             model.fit(\n",
    "#             {'main_input': X_train,'dep_input':X_train_dep},\n",
    "#             {'main_output': Y_train}, epochs = num_epochs, batch_size = batch_size,\n",
    "#             validation_data = (\n",
    "#                                 {'main_input': X_val, 'dep_input':X_val_dep},\n",
    "#                                 {'main_output': Y_val}\n",
    "#                                 ), callbacks=[tb,csv_logger,checkpoint])\n",
    "#     else:\n",
    "#       model.fit(\n",
    "#         {'main_input': X_train},\n",
    "#         {'main_output': Y_train}, epochs = num_epochs, batch_size = batch_size,\n",
    "#         validation_data = (\n",
    "#             {'main_input': X_val},\n",
    "#             {'main_output': Y_val}\n",
    "#         ), callbacks=[tb,csv_logger,checkpoint])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# use_pos=False\n",
    "# use_meta=True\n",
    "# use_dep=True\n",
    "#\n",
    "# # LSTM model\n",
    "# model_lstm = Sequential()\n",
    "# model_lstm.add(Embedding(vocab_length+1, hidden_size, input_length=num_steps))\n",
    "# model_lstm.add(Bidirectional(LSTM(hidden_size)))\n",
    "# model_lstm.add(Dense(6, activation='softmax'))\n",
    "#\n",
    "#\n",
    "# # statement embed LSTM\n",
    "# statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "# x = Embedding(vocab_length+1,EMBED_DIM,weights=[embedding_matrix],input_length=num_steps,trainable=False)(statement_input)\n",
    "# lstm_in = LSTM(lstm_size,dropout=0.2)(x)\n",
    "#\n",
    "#\n",
    "#\n",
    "# # pos embed LSTM\n",
    "# pos_input = Input(shape=(num_steps,), dtype='int32', name='pos_input')\n",
    "# x2 = Embedding(max(pos_dict.values()), max(pos_dict.values()), weights=[pos_embeddings], input_length=num_steps, trainable=False)(pos_input)\n",
    "# lstm_in2 = LSTM(lstm_size, dropout=0.2)(x2)\n",
    "#\n",
    "#\n",
    "# # dep embed LSTM\n",
    "# dep_input = Input(shape=(num_steps,), dtype='int32', name='dep_input')\n",
    "# x3 = Embedding(max(dep_dict.values()), max(dep_dict.values()), weights=[dep_embeddings], input_length=num_steps, trainable=False)(dep_input)\n",
    "# lstm_in3 = LSTM(lstm_size, dropout=0.2)(x3)\n",
    "#\n",
    "#\n",
    "# # meta data Dense\n",
    "# meta_input = Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "# x_meta = Dense(64, activation='relu')(meta_input)\n",
    "#\n",
    "#\n",
    "# if use_pos and use_meta:\n",
    "#   if use_dep:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in2, lstm_in3, x_meta])\n",
    "#   else:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in2, x_meta])\n",
    "# elif use_meta:\n",
    "#   if use_dep:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in3, x_meta])\n",
    "#   else:\n",
    "#     x = keras.layers.concatenate([lstm_in, x_meta])\n",
    "# elif use_pos:\n",
    "#   if use_dep:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in3, lstm_in2])\n",
    "#   else:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in2])\n",
    "# else:\n",
    "#   if use_dep:\n",
    "#     x = keras.layers.concatenate([lstm_in, lstm_in3])\n",
    "#   else:\n",
    "#     x = lstm_in\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "#\n",
    "# if use_pos and use_meta:\n",
    "#   if use_dep:\n",
    "#     model_lstm = Model(inputs=[statement_input, pos_input, dep_input, meta_input], outputs=[main_output])\n",
    "#   else:\n",
    "#     model_lstm = Model(inputs=[statement_input, pos_input, meta_input], outputs=[main_output])\n",
    "# elif use_meta:\n",
    "#   if use_dep:\n",
    "#     model_lstm = Model(inputs=[statement_input, dep_input, meta_input], outputs=[main_output])\n",
    "#   else:\n",
    "#     model_lstm = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "# elif use_pos:\n",
    "#   if use_dep:\n",
    "#     model_lstm = Model(inputs=[statement_input, dep_input, pos_input], outputs=[main_output])\n",
    "#   else:\n",
    "#     model_lstm = Model(inputs=[statement_input, pos_input], outputs=[main_output])\n",
    "# else:\n",
    "#   if use_dep:\n",
    "#     model_lstm = Model(inputs=[statement_input, dep_input], outputs=[main_output])\n",
    "#   else:\n",
    "#     model_lstm = Model(inputs=[statement_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
