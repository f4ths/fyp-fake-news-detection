{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,LSTM,Dropout,Bidirectional,GRU, Concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# from keras.utils import pad_sequences\n",
    "from keras_preprocessing import sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary dictionaries: 9606 9495\n",
      "Shape of the embedding matrixes: (9607, 100) (9496, 300)\n"
     ]
    }
   ],
   "source": [
    "## LOAD SAVED DATA\n",
    "# load data\n",
    "train_data = pd.read_pickle('processed_train_data_v2.p')\n",
    "val_data = pd.read_pickle('processed_val_data_v2.p')\n",
    "test_data = pd.read_pickle('processed_test_data_v2.p')\n",
    "\n",
    "# load vocab dicts\n",
    "vocabulary_dict_custom = pickle.load(open('vocabulary_statement_custom.p', 'rb'))\n",
    "vocabulary_dict_spacy = pickle.load(open('vocabulary_statement_spacy.p', 'rb'))\n",
    "# vocab_length = len(vocabulary_dict_custom)\n",
    "print(\"Length of the vocabulary dictionaries:\", len(vocabulary_dict_custom), len(vocabulary_dict_spacy))\n",
    "\n",
    "# load Glove embeddings matrixes\n",
    "embedding_matrix_custom_100d = np.load('embedding_matrix_custom_100d.npy')\n",
    "embedding_matrix_spacy_100d = np.load('embedding_matrix_spacy_100d.npy')\n",
    "embedding_matrix_custom_300d = np.load('embedding_matrix_custom_300d.npy')\n",
    "embedding_matrix_spacy_300d = np.load('embedding_matrix_spacy_300d.npy')\n",
    "print(\"Shape of the embedding matrixes:\", embedding_matrix_custom_100d.shape, embedding_matrix_spacy_300d.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "label_map = {'pants-fire':0, 'false':1, 'barely-true':2, 'half-true':3, 'mostly-true':4, 'true':5}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "train_data['output'] = train_data['label'].apply(lambda x: label_map[x])\n",
    "val_data['output'] = val_data['label'].apply(lambda x: label_map[x])\n",
    "test_data['output'] = test_data['label'].apply(lambda x: label_map[x])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "3    2114\n1    1995\n4    1962\n5    1676\n2    1654\n0     839\nName: output, dtype: int64"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['output'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load DEP_dict\n",
    "with open('dep_dict.p', 'rb') as f:\n",
    "    dep_dict_fine, dep_dict_custom = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load POS dict\n",
    "with open('pos_dicts.p', 'rb') as f:\n",
    "    pos_dict_custom, pos_dict_fine, pos_dict_fine_merged, pos_dict_default = pickle.load(f) #pos_dict_default is not used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "       index          id        label  \\\n0          0   2635.json        false   \n1          1  10540.json    half-true   \n2          2    324.json  mostly-true   \n3          3   1123.json        false   \n4          4   9028.json    half-true   \n...      ...         ...          ...   \n10235  10235   5473.json  mostly-true   \n10236  10236   3408.json  mostly-true   \n10237  10237   3959.json    half-true   \n10238  10238   2253.json        false   \n10239  10239   1155.json   pants-fire   \n\n                                               statement  \\\n0      Says the Annies List political group supports ...   \n1      When did the decline of coal start? It started...   \n2      Hillary Clinton agrees with John McCain \"by vo...   \n3      Health care reform legislation is likely to ma...   \n4      The economic turnaround started at the end of ...   \n...                                                  ...   \n10235  There are a larger number of shark attacks in ...   \n10236  Democrats have now become the party of the [At...   \n10237  Says an alternative to Social Security that op...   \n10238  On lifting the U.S. Cuban embargo and allowing...   \n10239  The Department of Veterans Affairs has a manua...   \n\n                                  subject         speaker  \\\n0                                abortion    dwayne-bohac   \n1      energy,history,job-accomplishments  scott-surovell   \n2                          foreign-policy    barack-obama   \n3                             health-care    blog-posting   \n4                            economy,jobs   charlie-crist   \n...                                   ...             ...   \n10235                   animals,elections    aclu-florida   \n10236                           elections     alan-powell   \n10237          retirement,social-security     herman-cain   \n10238              florida,foreign-policy     jeff-greene   \n10239                health-care,veterans  michael-steele   \n\n                                           job-title       party  \\\n0                               State representative  republican   \n1                                     State delegate    democrat   \n2                                          President    democrat   \n3                                            unknown        none   \n4                                            unknown    democrat   \n...                                              ...         ...   \n10235                                        unknown        none   \n10236                                        unknown  republican   \n10237                                        unknown  republican   \n10238                                        unknown    democrat   \n10239  chairman of the Republican National Committee  republican   \n\n       barely-true-counts  false-counts  ...  \\\n0                     0.0           1.0  ...   \n1                     0.0           0.0  ...   \n2                    70.0          71.0  ...   \n3                     7.0          19.0  ...   \n4                    15.0           9.0  ...   \n...                   ...           ...  ...   \n10235                 0.0           1.0  ...   \n10236                 0.0           0.0  ...   \n10237                 4.0          11.0  ...   \n10238                 3.0           1.0  ...   \n10239                 0.0           1.0  ...   \n\n                                      pos-id-fine-merged  \\\n0             [2, 5, 5, 3, 1, 3, 2, 1, 5, 3, 3, 5, 3, 5]   \n1      [5, 2, 5, 3, 5, 3, 3, 5, 5, 2, 5, 1, 3, 2, 5, ...   \n2      [3, 3, 2, 5, 3, 3, 5, 5, 2, 5, 2, 3, 3, 5, 3, ...   \n3                [3, 3, 3, 3, 2, 1, 5, 2, 1, 3, 3, 3, 5]   \n4                      [5, 1, 3, 2, 5, 5, 3, 5, 5, 3, 5]   \n...                                                  ...   \n10235  [5, 2, 5, 1, 3, 5, 3, 3, 5, 3, 5, 5, 2, 3, 5, ...   \n10236  [5, 2, 4, 2, 5, 3, 5, 5, 5, 3, 5, 3, 3, 5, 5, ...   \n10237  [2, 5, 3, 5, 3, 3, 5, 2, 5, 3, 3, 5, 3, 5, 2, ...   \n10238               [5, 2, 5, 3, 1, 3, 5, 2, 3, 5, 3, 5]   \n10239  [5, 3, 5, 5, 5, 2, 5, 3, 4, 4, 2, 5, 3, 3, 5, ...   \n\n                                           pos-id-custom  \\\n0          [2, 15, 9, 9, 3, 1, 2, 3, 11, 1, 1, 5, 1, 11]   \n1      [8, 2, 15, 1, 5, 1, 1, 11, 6, 2, 8, 3, 1, 2, 5...   \n2      [9, 9, 2, 5, 9, 9, 11, 5, 2, 13, 2, 9, 9, 15, ...   \n3             [1, 1, 1, 1, 12, 3, 13, 2, 3, 1, 1, 1, 11]   \n4                   [15, 3, 1, 2, 5, 15, 1, 5, 6, 1, 11]   \n...                                                  ...   \n10235  [6, 2, 15, 3, 1, 5, 1, 1, 5, 9, 8, 6, 2, 1, 5,...   \n10236  [9, 12, 4, 2, 15, 1, 5, 15, 15, 9, 11, 9, 1, 1...   \n10237  [2, 15, 1, 5, 9, 9, 6, 2, 5, 9, 9, 11, 9, 11, ...   \n10238            [5, 2, 15, 9, 3, 1, 16, 2, 1, 5, 9, 11]   \n10239  [15, 9, 5, 9, 9, 2, 15, 1, 4, 4, 2, 6, 1, 1, 5...   \n\n                                             dep-id-fine  \\\n0             [8, 8, 4, 4, 3, 1, 8, 3, 8, 7, 2, 8, 8, 8]   \n1      [8, 8, 8, 1, 8, 7, 8, 8, 1, 8, 8, 3, 1, 8, 8, ...   \n2      [7, 1, 8, 8, 7, 8, 8, 8, 8, 8, 8, 7, 8, 8, 2, ...   \n3                [7, 7, 7, 1, 8, 8, 8, 8, 3, 7, 7, 2, 8]   \n4                      [8, 3, 1, 8, 8, 8, 8, 8, 8, 8, 8]   \n...                                                  ...   \n10235  [8, 8, 8, 3, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, ...   \n10236  [1, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 7, 8, 8, 6, ...   \n10237  [8, 8, 1, 8, 7, 8, 1, 8, 8, 7, 8, 8, 8, 8, 8, ...   \n10238               [8, 8, 8, 4, 3, 2, 8, 6, 2, 8, 8, 8]   \n10239  [8, 1, 8, 7, 8, 8, 8, 2, 8, 8, 5, 8, 8, 2, 8, ...   \n\n                                           dep-id-custom  \\\n0          [7, 5, 11, 11, 8, 6, 11, 8, 1, 4, 9, 2, 3, 1]   \n1      [11, 7, 5, 6, 2, 4, 3, 1, 6, 7, 11, 8, 6, 11, ...   \n2      [4, 6, 7, 2, 4, 3, 1, 2, 11, 10, 11, 4, 11, 5,...   \n3             [4, 4, 4, 6, 7, 11, 10, 11, 8, 4, 4, 9, 1]   \n4                     [5, 8, 6, 7, 2, 5, 3, 2, 11, 3, 1]   \n...                                                  ...   \n10235  [11, 7, 5, 8, 11, 2, 4, 3, 2, 3, 11, 11, 11, 1...   \n10236  [6, 10, 11, 7, 5, 11, 2, 5, 11, 11, 1, 4, 3, 1...   \n10237  [7, 5, 6, 2, 4, 3, 6, 11, 2, 4, 3, 1, 11, 1, 1...   \n10238           [7, 11, 5, 11, 8, 9, 11, 11, 9, 2, 3, 1]   \n10239  [5, 6, 2, 4, 3, 7, 5, 9, 11, 11, 11, 11, 11, 9...   \n\n                                     statement-customswr  \\\n0      say annies list political group support third ...   \n1      when do decline coal start start when natural ...   \n2      hillary clinton agree john mccain vote give ge...   \n3      health care reform legislation be likely manda...   \n4                     economic turnaround start end term   \n...                                                  ...   \n10235  be large number shark attack florida than be c...   \n10236  democrats have now become party atlanta metro ...   \n10237  say alternative social security operate galves...   \n10238      lift u.s. cuban embargo and allow travel cuba   \n10239  department veterans affairs have manual out th...   \n\n                                      statement-spacyswr job-id-jaccard  \\\n0      say annies list political group support trimes...              2   \n1      decline coal start start natural gas take star...              6   \n2      hillary clinton agree john mccain vote george ...              2   \n3      health care reform legislation likely mandate ...              0   \n4                     economic turnaround start end term              0   \n...                                                  ...            ...   \n10235  large number shark attack florida case voter f...              0   \n10236           democrats party atlanta metro area black              0   \n10237  say alternative social security operate galves...              0   \n10238          lift u.s. cuban embargo allow travel cuba              0   \n10239  department veterans affairs manual tell vetera...              6   \n\n       party-id-jaccard context-id-jaccard subject-id-jaccard  \n0                     0                 13                  2  \n1                     1                  0                  5  \n2                     1                  3                  5  \n3                     2                  0                  0  \n4                     1                  3                  6  \n...                 ...                ...                ...  \n10235                 2                  3                  0  \n10236                 0                  3                  3  \n10237                 0                  0                  3  \n10238                 1                 13                  5  \n10239                 0                  3                  0  \n\n[10240 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>label</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>job-title</th>\n      <th>party</th>\n      <th>barely-true-counts</th>\n      <th>false-counts</th>\n      <th>...</th>\n      <th>pos-id-fine-merged</th>\n      <th>pos-id-custom</th>\n      <th>dep-id-fine</th>\n      <th>dep-id-custom</th>\n      <th>statement-customswr</th>\n      <th>statement-spacyswr</th>\n      <th>job-id-jaccard</th>\n      <th>party-id-jaccard</th>\n      <th>context-id-jaccard</th>\n      <th>subject-id-jaccard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2635.json</td>\n      <td>false</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>[2, 5, 5, 3, 1, 3, 2, 1, 5, 3, 3, 5, 3, 5]</td>\n      <td>[2, 15, 9, 9, 3, 1, 2, 3, 11, 1, 1, 5, 1, 11]</td>\n      <td>[8, 8, 4, 4, 3, 1, 8, 3, 8, 7, 2, 8, 8, 8]</td>\n      <td>[7, 5, 11, 11, 8, 6, 11, 8, 1, 4, 9, 2, 3, 1]</td>\n      <td>say annies list political group support third ...</td>\n      <td>say annies list political group support trimes...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>13</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10540.json</td>\n      <td>half-true</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>[5, 2, 5, 3, 5, 3, 3, 5, 5, 2, 5, 1, 3, 2, 5, ...</td>\n      <td>[8, 2, 15, 1, 5, 1, 1, 11, 6, 2, 8, 3, 1, 2, 5...</td>\n      <td>[8, 8, 8, 1, 8, 7, 8, 8, 1, 8, 8, 3, 1, 8, 8, ...</td>\n      <td>[11, 7, 5, 6, 2, 4, 3, 1, 6, 7, 11, 8, 6, 11, ...</td>\n      <td>when do decline coal start start when natural ...</td>\n      <td>decline coal start start natural gas take star...</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>324.json</td>\n      <td>mostly-true</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>...</td>\n      <td>[3, 3, 2, 5, 3, 3, 5, 5, 2, 5, 2, 3, 3, 5, 3, ...</td>\n      <td>[9, 9, 2, 5, 9, 9, 11, 5, 2, 13, 2, 9, 9, 15, ...</td>\n      <td>[7, 1, 8, 8, 7, 8, 8, 8, 8, 8, 8, 7, 8, 8, 2, ...</td>\n      <td>[4, 6, 7, 2, 4, 3, 1, 2, 11, 10, 11, 4, 11, 5,...</td>\n      <td>hillary clinton agree john mccain vote give ge...</td>\n      <td>hillary clinton agree john mccain vote george ...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1123.json</td>\n      <td>false</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td>unknown</td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>...</td>\n      <td>[3, 3, 3, 3, 2, 1, 5, 2, 1, 3, 3, 3, 5]</td>\n      <td>[1, 1, 1, 1, 12, 3, 13, 2, 3, 1, 1, 1, 11]</td>\n      <td>[7, 7, 7, 1, 8, 8, 8, 8, 3, 7, 7, 2, 8]</td>\n      <td>[4, 4, 4, 6, 7, 11, 10, 11, 8, 4, 4, 9, 1]</td>\n      <td>health care reform legislation be likely manda...</td>\n      <td>health care reform legislation likely mandate ...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9028.json</td>\n      <td>half-true</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td>unknown</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>[5, 1, 3, 2, 5, 5, 3, 5, 5, 3, 5]</td>\n      <td>[15, 3, 1, 2, 5, 15, 1, 5, 6, 1, 11]</td>\n      <td>[8, 3, 1, 8, 8, 8, 8, 8, 8, 8, 8]</td>\n      <td>[5, 8, 6, 7, 2, 5, 3, 2, 11, 3, 1]</td>\n      <td>economic turnaround start end term</td>\n      <td>economic turnaround start end term</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10235</th>\n      <td>10235</td>\n      <td>5473.json</td>\n      <td>mostly-true</td>\n      <td>There are a larger number of shark attacks in ...</td>\n      <td>animals,elections</td>\n      <td>aclu-florida</td>\n      <td>unknown</td>\n      <td>none</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>[5, 2, 5, 1, 3, 5, 3, 3, 5, 3, 5, 5, 2, 3, 5, ...</td>\n      <td>[6, 2, 15, 3, 1, 5, 1, 1, 5, 9, 8, 6, 2, 1, 5,...</td>\n      <td>[8, 8, 8, 3, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n      <td>[11, 7, 5, 8, 11, 2, 4, 3, 2, 3, 11, 11, 11, 1...</td>\n      <td>be large number shark attack florida than be c...</td>\n      <td>large number shark attack florida case voter f...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10236</th>\n      <td>10236</td>\n      <td>3408.json</td>\n      <td>mostly-true</td>\n      <td>Democrats have now become the party of the [At...</td>\n      <td>elections</td>\n      <td>alan-powell</td>\n      <td>unknown</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>[5, 2, 4, 2, 5, 3, 5, 5, 5, 3, 5, 3, 3, 5, 5, ...</td>\n      <td>[9, 12, 4, 2, 15, 1, 5, 15, 15, 9, 11, 9, 1, 1...</td>\n      <td>[1, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 7, 8, 8, 6, ...</td>\n      <td>[6, 10, 11, 7, 5, 11, 2, 5, 11, 11, 1, 4, 3, 1...</td>\n      <td>democrats have now become party atlanta metro ...</td>\n      <td>democrats party atlanta metro area black</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10237</th>\n      <td>10237</td>\n      <td>3959.json</td>\n      <td>half-true</td>\n      <td>Says an alternative to Social Security that op...</td>\n      <td>retirement,social-security</td>\n      <td>herman-cain</td>\n      <td>unknown</td>\n      <td>republican</td>\n      <td>4.0</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>[2, 5, 3, 5, 3, 3, 5, 2, 5, 3, 3, 5, 3, 5, 2, ...</td>\n      <td>[2, 15, 1, 5, 9, 9, 6, 2, 5, 9, 9, 11, 9, 11, ...</td>\n      <td>[8, 8, 1, 8, 7, 8, 1, 8, 8, 7, 8, 8, 8, 8, 8, ...</td>\n      <td>[7, 5, 6, 2, 4, 3, 6, 11, 2, 4, 3, 1, 11, 1, 1...</td>\n      <td>say alternative social security operate galves...</td>\n      <td>say alternative social security operate galves...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10238</th>\n      <td>10238</td>\n      <td>2253.json</td>\n      <td>false</td>\n      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n      <td>florida,foreign-policy</td>\n      <td>jeff-greene</td>\n      <td>unknown</td>\n      <td>democrat</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>[5, 2, 5, 3, 1, 3, 5, 2, 3, 5, 3, 5]</td>\n      <td>[5, 2, 15, 9, 3, 1, 16, 2, 1, 5, 9, 11]</td>\n      <td>[8, 8, 8, 4, 3, 2, 8, 6, 2, 8, 8, 8]</td>\n      <td>[7, 11, 5, 11, 8, 9, 11, 11, 9, 2, 3, 1]</td>\n      <td>lift u.s. cuban embargo and allow travel cuba</td>\n      <td>lift u.s. cuban embargo allow travel cuba</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10239</th>\n      <td>10239</td>\n      <td>1155.json</td>\n      <td>pants-fire</td>\n      <td>The Department of Veterans Affairs has a manua...</td>\n      <td>health-care,veterans</td>\n      <td>michael-steele</td>\n      <td>chairman of the Republican National Committee</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>[5, 3, 5, 5, 5, 2, 5, 3, 4, 4, 2, 5, 3, 3, 5, ...</td>\n      <td>[15, 9, 5, 9, 9, 2, 15, 1, 4, 4, 2, 6, 1, 1, 5...</td>\n      <td>[8, 1, 8, 7, 8, 8, 8, 2, 8, 8, 5, 8, 8, 2, 8, ...</td>\n      <td>[5, 6, 2, 4, 3, 7, 5, 9, 11, 11, 11, 11, 11, 9...</td>\n      <td>department veterans affairs have manual out th...</td>\n      <td>department veterans affairs manual tell vetera...</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10240 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_data = train_data.rename(columns={'job-title-id-kmeans': 'job-id-kmeans'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "val_data = val_data.rename(columns=\n",
    "                               {'preprocessed-context-clustered': 'context-id-kmeans',\n",
    "                                'preprocessed-job-title-clustered': 'job-id-kmeans',\n",
    "                                'preprocessed-party-clustered': 'party-id-kmeans',\n",
    "                                'preprocessed-subject-clustered': 'subject-id-kmeans'\n",
    "                                }\n",
    "                               )\n",
    "\n",
    "test_data = test_data.rename(columns=\n",
    "                               {'preprocessed-context-clustered': 'context-id-kmeans',\n",
    "                                'preprocessed-job-title-clustered': 'job-id-kmeans',\n",
    "                                'preprocessed-party-clustered': 'party-id-kmeans',\n",
    "                                'preprocessed-subject-clustered': 'subject-id-kmeans'\n",
    "                                }\n",
    "                               )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# EXPORT data as pickle\n",
    "train_data.to_pickle('processed_train_data_v2.p')\n",
    "val_data.to_pickle('processed_val_data_v2.p')\n",
    "test_data.to_pickle('processed_test_data_v2.p')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# IMPORT v2 data\n",
    "train_data = pd.read_pickle('processed_train_data_v2.p')\n",
    "val_data = pd.read_pickle('processed_val_data_v2.p')\n",
    "test_data = pd.read_pickle('processed_test_data_v2.p')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# EXPORT DATA as p\n",
    "train_data.to_pickle('processed_train_data_v2.p')\n",
    "val_data.to_pickle('processed_val_data_v2.p')\n",
    "test_data.to_pickle('processed_test_data_v2.p')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['index', 'id', 'label', 'statement', 'subject', 'speaker', 'job-title',\n       'party', 'barely-true-counts', 'false-counts', 'half-true-counts',\n       'mostly-true-counts', 'pants-on-fire-counts', 'context', 'output',\n       'preprocessed-subject', 'preprocessed-context', 'context-id-kmeans',\n       'preprocessed-job-title', 'preprocessed-party', 'party-id-kmeans',\n       'job-title-id-kmeans', 'subject-id-kmeans', 'pos-id-fine',\n       'pos-id-fine-merged', 'pos-id-custom', 'dep-id-fine', 'dep-id-custom',\n       'statement-customswr', 'statement-spacyswr', 'job-id-jaccard',\n       'party-id-jaccard', 'context-id-jaccard', 'subject-id-jaccard'],\n      dtype='object')"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['index', 'id', 'label', 'statement', 'subject', 'speaker', 'job-title',\n       'party', 'barely-true-counts', 'false-counts', 'half-true-counts',\n       'mostly-true-counts', 'pants-on-fire-counts', 'context', 'output',\n       'preprocessed-job-title', 'preprocessed-context',\n       'preprocessed-context-clustered', 'preprocessed-job-title-clustered',\n       'preprocessed-subject', 'preprocessed-subject-clustered',\n       'preprocessed-party', 'preprocessed-party-clustered', 'pos-id-fine',\n       'pos-id-fine-merged', 'pos-id-custom', 'dep-id-fine', 'dep-id-custom',\n       'statement-customswr', 'statement-spacyswr', 'job-id-jaccard',\n       'party-id-jaccard', 'context-id-jaccard', 'subject-id-jaccard'],\n      dtype='object')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['index', 'id', 'label', 'statement', 'subject', 'speaker', 'job-title',\n       'party', 'barely-true-counts', 'false-counts', 'half-true-counts',\n       'mostly-true-counts', 'pants-on-fire-counts', 'context', 'output',\n       'preprocessed-job-title', 'preprocessed-context', 'context-id-kmeans',\n       'job-id-kmeans', 'preprocessed-subject', 'subject-id-kmeans',\n       'preprocessed-party', 'party-id-kmeans', 'pos-id-fine',\n       'pos-id-fine-merged', 'pos-id-custom', 'dep-id-fine', 'dep-id-custom',\n       'statement-customswr', 'statement-spacyswr', 'job-id-jaccard',\n       'party-id-jaccard', 'context-id-jaccard', 'subject-id-jaccard'],\n      dtype='object')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "## OUTPUT\n",
    "Y_train = train_data['output']\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=6)\n",
    "\n",
    "Y_val = val_data['output']\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes=6)\n",
    "\n",
    "Y_test = test_data['output']\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# One-hot encode the metadata columns\n",
    "\n",
    "# train sets\n",
    "context_id_kmeans_encoded = to_categorical(train_data['context-id-kmeans'])\n",
    "subject_id_kmeans_encoded = to_categorical(train_data['subject-id-kmeans'])\n",
    "job_id_kmeans_encoded = to_categorical(train_data['job-id-kmeans'])\n",
    "party_id_kmeans_encoded = to_categorical(train_data['party-id-kmeans'])\n",
    "\n",
    "context_id_jaccard_encoded = to_categorical(train_data['context-id-jaccard'])\n",
    "subject_id_jaccard_encoded = to_categorical(train_data['subject-id-jaccard'])\n",
    "job_id_jaccard_encoded = to_categorical(train_data['job-id-jaccard'])\n",
    "party_id_jaccard_encoded = to_categorical(train_data['party-id-jaccard'])\n",
    "\n",
    "# validation sets\n",
    "val_context_id_kmeans_encoded = to_categorical(val_data['context-id-kmeans'])\n",
    "val_subject_id_kmeans_encoded = to_categorical(val_data['subject-id-kmeans'])\n",
    "val_job_id_kmeans_encoded = to_categorical(val_data['job-id-kmeans'])\n",
    "val_party_id_kmeans_encoded = to_categorical(val_data['party-id-kmeans'])\n",
    "\n",
    "val_context_id_jaccard_encoded = to_categorical(val_data['context-id-jaccard'])\n",
    "val_subject_id_jaccard_encoded = to_categorical(val_data['subject-id-jaccard'])\n",
    "val_job_id_jaccard_encoded = to_categorical(val_data['job-id-jaccard'])\n",
    "val_party_id_jaccard_encoded = to_categorical(val_data['party-id-jaccard'])\n",
    "\n",
    "# test sets\n",
    "test_context_id_kmeans_encoded = to_categorical(test_data['context-id-kmeans'])\n",
    "test_subject_id_kmeans_encoded = to_categorical(test_data['subject-id-kmeans'])\n",
    "test_job_id_kmeans_encoded = to_categorical(test_data['job-id-kmeans'])\n",
    "test_party_id_kmeans_encoded = to_categorical(test_data['party-id-kmeans'])\n",
    "\n",
    "test_context_id_jaccard_encoded = to_categorical(test_data['context-id-jaccard'])\n",
    "test_subject_id_jaccard_encoded = to_categorical(test_data['subject-id-jaccard'])\n",
    "test_job_id_jaccard_encoded = to_categorical(test_data['job-id-jaccard'])\n",
    "test_party_id_jaccard_encoded = to_categorical(test_data['party-id-jaccard'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# Concatenate the one-hot encoded categorical columns for each comparison case:\n",
    "# K-means comparison\n",
    "X_train_meta_kmeans = np.hstack((context_id_kmeans_encoded, subject_id_kmeans_encoded, job_id_kmeans_encoded, party_id_kmeans_encoded))\n",
    "X_val_meta_kmeans = np.hstack((val_context_id_kmeans_encoded, val_subject_id_kmeans_encoded, val_job_id_kmeans_encoded, val_party_id_kmeans_encoded))\n",
    "X_test_meta_kmeans = np.hstack((test_context_id_kmeans_encoded, test_subject_id_kmeans_encoded, test_job_id_kmeans_encoded, test_party_id_kmeans_encoded))\n",
    "\n",
    "# Jaccard comparison\n",
    "X_train_meta_jaccard = np.hstack((context_id_jaccard_encoded, subject_id_jaccard_encoded, job_id_jaccard_encoded, party_id_jaccard_encoded))\n",
    "X_val_meta_jaccard = np.hstack((val_context_id_jaccard_encoded, val_subject_id_jaccard_encoded, val_job_id_jaccard_encoded, val_party_id_jaccard_encoded))\n",
    "X_test_meta_jaccard = np.hstack((test_context_id_jaccard_encoded, test_subject_id_jaccard_encoded, test_job_id_jaccard_encoded, test_party_id_jaccard_encoded))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Dense Model for non-statement features (POS, DEP, metadata)\n",
    "# def create_dense_model(input_shape, num_classes):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     dense = Dense(512, activation='relu')(inputs)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dropout(0.5)(dense)\n",
    "#     dense = Dense(256, activation='relu')(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "#     dense = Dense(64, activation='relu')(dense)\n",
    "#     dense = Dropout(0.5)(dense)\n",
    "#     output = Dense(num_classes, activation='softmax')(dense)\n",
    "#     model = Model(inputs=inputs, outputs=output)\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def create_dense_model(input_shape, num_classes, learning_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    dense = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    output = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))(dense)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create ModelCheckpoint callback\n",
    "model_checkpoint_jaccard = ModelCheckpoint(filepath='best_model_jaccard.h5', monitor='val_loss', save_best_only=True)\n",
    "model_checkpoint_kmeans = ModelCheckpoint(filepath='best_model_kmeans.h5', monitor='val_loss', save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-MEANS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 12ms/step - loss: 2.5710 - accuracy: 0.1943 - val_loss: 2.2704 - val_accuracy: 0.1931\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.1009 - accuracy: 0.1960 - val_loss: 1.9909 - val_accuracy: 0.2157\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.9117 - accuracy: 0.2158 - val_loss: 1.8935 - val_accuracy: 0.2134\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.8397 - accuracy: 0.2172 - val_loss: 1.8354 - val_accuracy: 0.2375\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.8058 - accuracy: 0.2237 - val_loss: 1.8160 - val_accuracy: 0.2204\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.7890 - accuracy: 0.2218 - val_loss: 1.8091 - val_accuracy: 0.2235\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.7791 - accuracy: 0.2196 - val_loss: 1.7987 - val_accuracy: 0.2173\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.7723 - accuracy: 0.2218 - val_loss: 1.7940 - val_accuracy: 0.2266\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.7702 - accuracy: 0.2283 - val_loss: 1.7885 - val_accuracy: 0.2188\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7633 - accuracy: 0.2234 - val_loss: 1.7849 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7614 - accuracy: 0.2240 - val_loss: 1.7795 - val_accuracy: 0.2134\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7579 - accuracy: 0.2262 - val_loss: 1.7859 - val_accuracy: 0.2212\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7561 - accuracy: 0.2258 - val_loss: 1.7819 - val_accuracy: 0.2251\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7529 - accuracy: 0.2344 - val_loss: 1.7824 - val_accuracy: 0.2188\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7543 - accuracy: 0.2251 - val_loss: 1.7837 - val_accuracy: 0.2235\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7520 - accuracy: 0.2282 - val_loss: 1.7854 - val_accuracy: 0.2188\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7510 - accuracy: 0.2258 - val_loss: 1.7809 - val_accuracy: 0.2181\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7485 - accuracy: 0.2289 - val_loss: 1.7845 - val_accuracy: 0.2173\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7475 - accuracy: 0.2341 - val_loss: 1.7806 - val_accuracy: 0.2150\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7482 - accuracy: 0.2324 - val_loss: 1.7864 - val_accuracy: 0.2181\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7454 - accuracy: 0.2358 - val_loss: 1.7863 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffc551e2e0>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 6\n",
    "# First training with learning_rate = 0.0005\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "learning_rate = 0.0005\n",
    "dense_model_kmeans = create_dense_model(X_train_meta_kmeans.shape[1], num_classes, learning_rate)\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping, model_checkpoint_kmeans])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 8ms/step - loss: 2.6137 - accuracy: 0.1997 - val_loss: 2.3408 - val_accuracy: 0.1931\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.1733 - accuracy: 0.2125 - val_loss: 2.0611 - val_accuracy: 0.2142\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.9764 - accuracy: 0.2183 - val_loss: 1.9325 - val_accuracy: 0.2305\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8861 - accuracy: 0.2166 - val_loss: 1.8772 - val_accuracy: 0.2204\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8384 - accuracy: 0.2216 - val_loss: 1.8461 - val_accuracy: 0.2040\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8112 - accuracy: 0.2249 - val_loss: 1.8221 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7964 - accuracy: 0.2229 - val_loss: 1.8211 - val_accuracy: 0.2188\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7849 - accuracy: 0.2270 - val_loss: 1.8069 - val_accuracy: 0.2212\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7787 - accuracy: 0.2237 - val_loss: 1.8054 - val_accuracy: 0.2235\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7723 - accuracy: 0.2281 - val_loss: 1.8008 - val_accuracy: 0.2150\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7680 - accuracy: 0.2296 - val_loss: 1.8025 - val_accuracy: 0.2204\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7653 - accuracy: 0.2228 - val_loss: 1.7918 - val_accuracy: 0.2251\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7636 - accuracy: 0.2268 - val_loss: 1.7986 - val_accuracy: 0.2181\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7592 - accuracy: 0.2242 - val_loss: 1.8073 - val_accuracy: 0.2134\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7577 - accuracy: 0.2290 - val_loss: 1.7849 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7559 - accuracy: 0.2271 - val_loss: 1.7879 - val_accuracy: 0.2243\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7552 - accuracy: 0.2248 - val_loss: 1.7911 - val_accuracy: 0.2290\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7513 - accuracy: 0.2273 - val_loss: 1.7886 - val_accuracy: 0.2282\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7501 - accuracy: 0.2332 - val_loss: 1.7903 - val_accuracy: 0.2259\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7481 - accuracy: 0.2349 - val_loss: 1.7876 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7490 - accuracy: 0.2330 - val_loss: 1.7784 - val_accuracy: 0.2142\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7474 - accuracy: 0.2331 - val_loss: 1.7948 - val_accuracy: 0.2173\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7467 - accuracy: 0.2340 - val_loss: 1.7887 - val_accuracy: 0.2142\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7446 - accuracy: 0.2359 - val_loss: 1.7899 - val_accuracy: 0.2204\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7455 - accuracy: 0.2319 - val_loss: 1.7868 - val_accuracy: 0.2212\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7449 - accuracy: 0.2348 - val_loss: 1.7859 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7429 - accuracy: 0.2397 - val_loss: 1.7825 - val_accuracy: 0.2173\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7432 - accuracy: 0.2340 - val_loss: 1.7841 - val_accuracy: 0.2111\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7433 - accuracy: 0.2310 - val_loss: 1.7908 - val_accuracy: 0.2181\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7418 - accuracy: 0.2375 - val_loss: 1.7907 - val_accuracy: 0.2142\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7396 - accuracy: 0.2383 - val_loss: 1.7895 - val_accuracy: 0.2126\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffcb296d60>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 6\n",
    "# First training with learning_rate = 0.0005\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "learning_rate = 0.0005\n",
    "dense_model_kmeans = create_dense_model(X_train_meta_kmeans.shape[1], num_classes, learning_rate)\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping, model_checkpoint_kmeans])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model from the saved checkpoint\n",
    "best_model_kmeans = load_model('best_model_kmeans.h5')\n",
    "\n",
    "# Make predictions using the best model\n",
    "predictions = best_model_kmeans.predict(X_test_meta_kmeans)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have the true labels of the test set in a variable called `Y_test`\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Test set accuracy: {accuracy:.2f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step\n",
      "[[  0  38   0  15  39   0]\n",
      " [  0 138   0  53  58   0]\n",
      " [  0 118   0  52  42   0]\n",
      " [  0 150   0  72  43   0]\n",
      " [  0 120   0  83  38   0]\n",
      " [  0 130   0  40  38   0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model from the saved checkpoint\n",
    "best_model_kmeans = load_model('best_model_kmeans.h5')\n",
    "\n",
    "# Make predictions using the best model\n",
    "predictions = best_model_kmeans.predict(X_test_meta_kmeans)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the true labels of the test set in a variable called `Y_test`\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# # Calculate the accuracy\n",
    "# accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "# print(f'Test set accuracy: {accuracy:.2f}')\n",
    "#\n",
    "# # Calculate the precision (use `average` parameter to specify the type of averaging you want)\n",
    "# precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "# print(f'Test set precision: {precision:.2f}')\n",
    "#\n",
    "# # Calculate the recall (use `average` parameter to specify the type of averaging you want)\n",
    "# recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "# print(f'Test set recall: {recall:.2f}')\n",
    "#\n",
    "# # Calculate the F1 score (use `average` parameter to specify the type of averaging you want)\n",
    "# f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "# print(f'Test set F1 score: {f1:.2f}')\n",
    "\n",
    "# # Make predictions using the best model\n",
    "# predictions = best_model_kmeans.predict(X_test_meta_kmeans)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "# predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "Y_test = test_data['output']\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=6)\n",
    "\n",
    "# true_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# # Plot the confusion matrix using seaborn\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "# plt.xlabel('Predicted labels')\n",
    "# plt.ylabel('True labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "3    265\n1    249\n4    241\n2    212\n5    208\n0     92\nName: output, dtype: int64"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['output'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step\n",
      "[[  0  23 194  48   0]\n",
      " [  0  17 184  40   0]\n",
      " [  0  45 190  65   0]\n",
      " [  0  31 181  37   0]\n",
      " [  0  23 144  45   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predictions from the model\n",
    "y_pred = np.argmax(dense_model_kmeans.predict(X_test_meta_kmeans), axis=-1)\n",
    "\n",
    "# Convert the one-hot encoded labels back to their original format\n",
    "y_true = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(conf_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 2.2787 - accuracy: 0.2217 - val_loss: 1.8951 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7942 - accuracy: 0.2180 - val_loss: 1.7240 - val_accuracy: 0.2220\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6952 - accuracy: 0.2342 - val_loss: 1.6763 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6631 - accuracy: 0.2315 - val_loss: 1.6564 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6479 - accuracy: 0.2284 - val_loss: 1.6441 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6379 - accuracy: 0.2361 - val_loss: 1.6380 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6324 - accuracy: 0.2342 - val_loss: 1.6357 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6269 - accuracy: 0.2385 - val_loss: 1.6317 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6239 - accuracy: 0.2418 - val_loss: 1.6295 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6222 - accuracy: 0.2409 - val_loss: 1.6280 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6205 - accuracy: 0.2452 - val_loss: 1.6265 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6182 - accuracy: 0.2441 - val_loss: 1.6255 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6170 - accuracy: 0.2438 - val_loss: 1.6230 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6158 - accuracy: 0.2469 - val_loss: 1.6262 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6152 - accuracy: 0.2445 - val_loss: 1.6214 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6148 - accuracy: 0.2459 - val_loss: 1.6230 - val_accuracy: 0.2220\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6131 - accuracy: 0.2450 - val_loss: 1.6217 - val_accuracy: 0.2220\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6125 - accuracy: 0.2461 - val_loss: 1.6207 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6120 - accuracy: 0.2449 - val_loss: 1.6198 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6114 - accuracy: 0.2458 - val_loss: 1.6186 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6101 - accuracy: 0.2460 - val_loss: 1.6184 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6111 - accuracy: 0.2455 - val_loss: 1.6246 - val_accuracy: 0.2220\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6109 - accuracy: 0.2456 - val_loss: 1.6183 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6089 - accuracy: 0.2460 - val_loss: 1.6200 - val_accuracy: 0.2220\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6101 - accuracy: 0.2426 - val_loss: 1.6186 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2455 - val_loss: 1.6191 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6090 - accuracy: 0.2456 - val_loss: 1.6170 - val_accuracy: 0.2220\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2447 - val_loss: 1.6171 - val_accuracy: 0.2220\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6083 - accuracy: 0.2455 - val_loss: 1.6190 - val_accuracy: 0.2220\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6084 - accuracy: 0.2438 - val_loss: 1.6169 - val_accuracy: 0.2220\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6076 - accuracy: 0.2448 - val_loss: 1.6156 - val_accuracy: 0.2220\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6078 - accuracy: 0.2452 - val_loss: 1.6172 - val_accuracy: 0.2220\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2456 - val_loss: 1.6150 - val_accuracy: 0.2220\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.2455 - val_loss: 1.6170 - val_accuracy: 0.2220\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.2455 - val_loss: 1.6162 - val_accuracy: 0.2220\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.2456 - val_loss: 1.6153 - val_accuracy: 0.2220\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6070 - accuracy: 0.2455 - val_loss: 1.6150 - val_accuracy: 0.2220\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6068 - accuracy: 0.2456 - val_loss: 1.6163 - val_accuracy: 0.2220\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6070 - accuracy: 0.2453 - val_loss: 1.6177 - val_accuracy: 0.2220\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6070 - accuracy: 0.2457 - val_loss: 1.6188 - val_accuracy: 0.2220\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6070 - accuracy: 0.2456 - val_loss: 1.6153 - val_accuracy: 0.2220\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6057 - accuracy: 0.2459 - val_loss: 1.6145 - val_accuracy: 0.2220\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6061 - accuracy: 0.2447 - val_loss: 1.6152 - val_accuracy: 0.2220\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6067 - accuracy: 0.2457 - val_loss: 1.6143 - val_accuracy: 0.2220\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6063 - accuracy: 0.2452 - val_loss: 1.6134 - val_accuracy: 0.2220\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6061 - accuracy: 0.2460 - val_loss: 1.6131 - val_accuracy: 0.2220\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6059 - accuracy: 0.2454 - val_loss: 1.6140 - val_accuracy: 0.2220\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6063 - accuracy: 0.2456 - val_loss: 1.6176 - val_accuracy: 0.2220\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6053 - accuracy: 0.2456 - val_loss: 1.6144 - val_accuracy: 0.2220\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6058 - accuracy: 0.2459 - val_loss: 1.6155 - val_accuracy: 0.2220\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6058 - accuracy: 0.2453 - val_loss: 1.6153 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffbde47fa0>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second training with learning_rate = 0.001\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "learning_rate = 0.001\n",
    "dense_model_kmeans = create_dense_model(X_train_meta_kmeans.shape[1], num_classes, learning_rate) # Create a new model with fresh weights and updated learning rate\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping, model_checkpoint_kmeans])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "0        [9, 16, 16, 12, 1, 10, 9, 1, 16, 10, 11, 16, 1...\n1        [16, 5, 16, 10, 16, 10, 10, 16, 16, 5, 16, 1, ...\n2        [12, 12, 9, 16, 12, 12, 16, 16, 6, 16, 4, 12, ...\n3         [10, 10, 10, 10, 9, 1, 16, 4, 1, 10, 10, 11, 16]\n4               [16, 1, 10, 5, 16, 16, 10, 16, 16, 10, 16]\n                               ...                        \n10235    [16, 8, 16, 2, 10, 16, 10, 11, 16, 12, 16, 16,...\n10236    [16, 8, 13, 7, 16, 10, 16, 16, 16, 12, 16, 12,...\n10237    [9, 16, 10, 16, 12, 12, 16, 9, 16, 12, 12, 16,...\n10238        [16, 6, 16, 12, 1, 10, 16, 6, 10, 16, 12, 16]\n10239    [16, 12, 16, 16, 16, 9, 16, 10, 13, 13, 6, 16,...\nName: pos-id-fine, Length: 10240, dtype: object"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['pos-id-fine']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JACCARD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 8ms/step - loss: 2.5618 - accuracy: 0.2093 - val_loss: 2.2627 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 2.1135 - accuracy: 0.2168 - val_loss: 1.9631 - val_accuracy: 0.2188\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.9002 - accuracy: 0.2261 - val_loss: 1.8179 - val_accuracy: 0.2212\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7927 - accuracy: 0.2247 - val_loss: 1.7491 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7390 - accuracy: 0.2348 - val_loss: 1.7118 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7032 - accuracy: 0.2336 - val_loss: 1.6862 - val_accuracy: 0.2235\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6833 - accuracy: 0.2363 - val_loss: 1.6700 - val_accuracy: 0.2368\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6678 - accuracy: 0.2408 - val_loss: 1.6608 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6582 - accuracy: 0.2376 - val_loss: 1.6576 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6511 - accuracy: 0.2439 - val_loss: 1.6490 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6464 - accuracy: 0.2399 - val_loss: 1.6440 - val_accuracy: 0.2321\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6393 - accuracy: 0.2463 - val_loss: 1.6408 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6378 - accuracy: 0.2446 - val_loss: 1.6396 - val_accuracy: 0.2375\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6351 - accuracy: 0.2482 - val_loss: 1.6359 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6317 - accuracy: 0.2414 - val_loss: 1.6354 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6301 - accuracy: 0.2472 - val_loss: 1.6338 - val_accuracy: 0.2329\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6284 - accuracy: 0.2451 - val_loss: 1.6316 - val_accuracy: 0.2407\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6264 - accuracy: 0.2468 - val_loss: 1.6299 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6259 - accuracy: 0.2446 - val_loss: 1.6301 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6249 - accuracy: 0.2451 - val_loss: 1.6294 - val_accuracy: 0.2407\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6228 - accuracy: 0.2470 - val_loss: 1.6264 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6224 - accuracy: 0.2470 - val_loss: 1.6248 - val_accuracy: 0.2438\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6221 - accuracy: 0.2440 - val_loss: 1.6265 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6204 - accuracy: 0.2481 - val_loss: 1.6320 - val_accuracy: 0.2329\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6202 - accuracy: 0.2479 - val_loss: 1.6241 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6189 - accuracy: 0.2481 - val_loss: 1.6251 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6183 - accuracy: 0.2433 - val_loss: 1.6238 - val_accuracy: 0.2399\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6173 - accuracy: 0.2449 - val_loss: 1.6253 - val_accuracy: 0.2220\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6179 - accuracy: 0.2461 - val_loss: 1.6235 - val_accuracy: 0.2220\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6168 - accuracy: 0.2446 - val_loss: 1.6212 - val_accuracy: 0.2220\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6168 - accuracy: 0.2441 - val_loss: 1.6231 - val_accuracy: 0.2220\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6164 - accuracy: 0.2444 - val_loss: 1.6202 - val_accuracy: 0.2220\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6138 - accuracy: 0.2469 - val_loss: 1.6193 - val_accuracy: 0.2329\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6153 - accuracy: 0.2463 - val_loss: 1.6192 - val_accuracy: 0.2220\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6144 - accuracy: 0.2472 - val_loss: 1.6194 - val_accuracy: 0.2220\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6136 - accuracy: 0.2446 - val_loss: 1.6195 - val_accuracy: 0.2220\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6142 - accuracy: 0.2416 - val_loss: 1.6194 - val_accuracy: 0.2220\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6116 - accuracy: 0.2496 - val_loss: 1.6199 - val_accuracy: 0.2399\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6125 - accuracy: 0.2441 - val_loss: 1.6202 - val_accuracy: 0.2220\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6125 - accuracy: 0.2442 - val_loss: 1.6186 - val_accuracy: 0.2220\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6132 - accuracy: 0.2484 - val_loss: 1.6187 - val_accuracy: 0.2220\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6133 - accuracy: 0.2416 - val_loss: 1.6201 - val_accuracy: 0.2360\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6109 - accuracy: 0.2468 - val_loss: 1.6183 - val_accuracy: 0.2220\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6120 - accuracy: 0.2427 - val_loss: 1.6192 - val_accuracy: 0.2282\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6115 - accuracy: 0.2436 - val_loss: 1.6169 - val_accuracy: 0.2235\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6105 - accuracy: 0.2485 - val_loss: 1.6178 - val_accuracy: 0.2220\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6108 - accuracy: 0.2442 - val_loss: 1.6164 - val_accuracy: 0.2220\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6115 - accuracy: 0.2439 - val_loss: 1.6165 - val_accuracy: 0.2220\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6100 - accuracy: 0.2450 - val_loss: 1.6149 - val_accuracy: 0.2212\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6104 - accuracy: 0.2484 - val_loss: 1.6178 - val_accuracy: 0.2220\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6098 - accuracy: 0.2440 - val_loss: 1.6205 - val_accuracy: 0.2430\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6099 - accuracy: 0.2467 - val_loss: 1.6160 - val_accuracy: 0.2220\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6071 - accuracy: 0.2496 - val_loss: 1.6156 - val_accuracy: 0.2212\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6093 - accuracy: 0.2480 - val_loss: 1.6137 - val_accuracy: 0.2212\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6089 - accuracy: 0.2467 - val_loss: 1.6138 - val_accuracy: 0.2212\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6092 - accuracy: 0.2491 - val_loss: 1.6167 - val_accuracy: 0.2212\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6085 - accuracy: 0.2472 - val_loss: 1.6156 - val_accuracy: 0.2220\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6092 - accuracy: 0.2431 - val_loss: 1.6140 - val_accuracy: 0.2220\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6084 - accuracy: 0.2452 - val_loss: 1.6162 - val_accuracy: 0.2220\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6076 - accuracy: 0.2447 - val_loss: 1.6181 - val_accuracy: 0.2407\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6082 - accuracy: 0.2474 - val_loss: 1.6143 - val_accuracy: 0.2220\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6079 - accuracy: 0.2468 - val_loss: 1.6133 - val_accuracy: 0.2259\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6072 - accuracy: 0.2474 - val_loss: 1.6158 - val_accuracy: 0.2220\n",
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6073 - accuracy: 0.2451 - val_loss: 1.6152 - val_accuracy: 0.2212\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6080 - accuracy: 0.2423 - val_loss: 1.6166 - val_accuracy: 0.2212\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6079 - accuracy: 0.2465 - val_loss: 1.6133 - val_accuracy: 0.2220\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6072 - accuracy: 0.2459 - val_loss: 1.6150 - val_accuracy: 0.2445\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6061 - accuracy: 0.2483 - val_loss: 1.6137 - val_accuracy: 0.2220\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6066 - accuracy: 0.2460 - val_loss: 1.6128 - val_accuracy: 0.2220\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6057 - accuracy: 0.2481 - val_loss: 1.6137 - val_accuracy: 0.2212\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6068 - accuracy: 0.2493 - val_loss: 1.6143 - val_accuracy: 0.2321\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6060 - accuracy: 0.2496 - val_loss: 1.6127 - val_accuracy: 0.2212\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6072 - accuracy: 0.2458 - val_loss: 1.6151 - val_accuracy: 0.2336\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6062 - accuracy: 0.2458 - val_loss: 1.6133 - val_accuracy: 0.2220\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6057 - accuracy: 0.2502 - val_loss: 1.6139 - val_accuracy: 0.2383\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6074 - accuracy: 0.2439 - val_loss: 1.6124 - val_accuracy: 0.2298\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6074 - accuracy: 0.2449 - val_loss: 1.6122 - val_accuracy: 0.2282\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6058 - accuracy: 0.2437 - val_loss: 1.6138 - val_accuracy: 0.2220\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6064 - accuracy: 0.2467 - val_loss: 1.6132 - val_accuracy: 0.2212\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6046 - accuracy: 0.2482 - val_loss: 1.6110 - val_accuracy: 0.2220\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6052 - accuracy: 0.2457 - val_loss: 1.6133 - val_accuracy: 0.2461\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6052 - accuracy: 0.2462 - val_loss: 1.6115 - val_accuracy: 0.2212\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6044 - accuracy: 0.2455 - val_loss: 1.6117 - val_accuracy: 0.2282\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6050 - accuracy: 0.2451 - val_loss: 1.6114 - val_accuracy: 0.2298\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6060 - accuracy: 0.2439 - val_loss: 1.6115 - val_accuracy: 0.2220\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6051 - accuracy: 0.2472 - val_loss: 1.6111 - val_accuracy: 0.2220\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6054 - accuracy: 0.2451 - val_loss: 1.6137 - val_accuracy: 0.2220\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6049 - accuracy: 0.2497 - val_loss: 1.6116 - val_accuracy: 0.2220\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6049 - accuracy: 0.2458 - val_loss: 1.6121 - val_accuracy: 0.2220\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6049 - accuracy: 0.2432 - val_loss: 1.6109 - val_accuracy: 0.2220\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6051 - accuracy: 0.2467 - val_loss: 1.6158 - val_accuracy: 0.2305\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6048 - accuracy: 0.2460 - val_loss: 1.6116 - val_accuracy: 0.2220\n",
      "Epoch 93/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6045 - accuracy: 0.2472 - val_loss: 1.6102 - val_accuracy: 0.2212\n",
      "Epoch 94/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6040 - accuracy: 0.2456 - val_loss: 1.6100 - val_accuracy: 0.2220\n",
      "Epoch 95/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6043 - accuracy: 0.2470 - val_loss: 1.6120 - val_accuracy: 0.2220\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6046 - accuracy: 0.2432 - val_loss: 1.6107 - val_accuracy: 0.2220\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6028 - accuracy: 0.2475 - val_loss: 1.6115 - val_accuracy: 0.2220\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6042 - accuracy: 0.2459 - val_loss: 1.6132 - val_accuracy: 0.2430\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6035 - accuracy: 0.2473 - val_loss: 1.6134 - val_accuracy: 0.2329\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6044 - accuracy: 0.2513 - val_loss: 1.6109 - val_accuracy: 0.2220\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6037 - accuracy: 0.2447 - val_loss: 1.6111 - val_accuracy: 0.2220\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6028 - accuracy: 0.2499 - val_loss: 1.6151 - val_accuracy: 0.2407\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.6037 - accuracy: 0.2514 - val_loss: 1.6106 - val_accuracy: 0.2220\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6027 - accuracy: 0.2464 - val_loss: 1.6104 - val_accuracy: 0.2212\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffbf776b80>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First training with learning_rate = 0.0005\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "learning_rate = 0.0005\n",
    "dense_model_jaccard = create_dense_model(X_train_meta_jaccard.shape[1], num_classes, learning_rate)\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val), callbacks=[early_stopping, model_checkpoint_jaccard])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "80/80 [==============================] - 2s 9ms/step - loss: 2.3710 - accuracy: 0.1940 - val_loss: 2.0040 - val_accuracy: 0.2181\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8989 - accuracy: 0.2107 - val_loss: 1.8313 - val_accuracy: 0.2336\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.8089 - accuracy: 0.2197 - val_loss: 1.7951 - val_accuracy: 0.2188\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7837 - accuracy: 0.2198 - val_loss: 1.7764 - val_accuracy: 0.2305\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7710 - accuracy: 0.2173 - val_loss: 1.7681 - val_accuracy: 0.2321\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7649 - accuracy: 0.2149 - val_loss: 1.7644 - val_accuracy: 0.2383\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7607 - accuracy: 0.2296 - val_loss: 1.7606 - val_accuracy: 0.2305\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7583 - accuracy: 0.2221 - val_loss: 1.7582 - val_accuracy: 0.2321\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7569 - accuracy: 0.2224 - val_loss: 1.7568 - val_accuracy: 0.2368\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7515 - accuracy: 0.2298 - val_loss: 1.7582 - val_accuracy: 0.2290\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7538 - accuracy: 0.2244 - val_loss: 1.7599 - val_accuracy: 0.2274\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7511 - accuracy: 0.2300 - val_loss: 1.7523 - val_accuracy: 0.2352\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7518 - accuracy: 0.2239 - val_loss: 1.7511 - val_accuracy: 0.2274\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7493 - accuracy: 0.2275 - val_loss: 1.7548 - val_accuracy: 0.2243\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7484 - accuracy: 0.2249 - val_loss: 1.7544 - val_accuracy: 0.2305\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7472 - accuracy: 0.2250 - val_loss: 1.7539 - val_accuracy: 0.2344\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7471 - accuracy: 0.2271 - val_loss: 1.7518 - val_accuracy: 0.2298\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7458 - accuracy: 0.2322 - val_loss: 1.7481 - val_accuracy: 0.2298\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7467 - accuracy: 0.2316 - val_loss: 1.7504 - val_accuracy: 0.2321\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7456 - accuracy: 0.2274 - val_loss: 1.7491 - val_accuracy: 0.2329\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7449 - accuracy: 0.2315 - val_loss: 1.7488 - val_accuracy: 0.2391\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7449 - accuracy: 0.2328 - val_loss: 1.7484 - val_accuracy: 0.2360\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7441 - accuracy: 0.2341 - val_loss: 1.7498 - val_accuracy: 0.2344\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7446 - accuracy: 0.2306 - val_loss: 1.7473 - val_accuracy: 0.2360\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7419 - accuracy: 0.2306 - val_loss: 1.7471 - val_accuracy: 0.2282\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7431 - accuracy: 0.2326 - val_loss: 1.7481 - val_accuracy: 0.2430\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7432 - accuracy: 0.2360 - val_loss: 1.7482 - val_accuracy: 0.2344\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7425 - accuracy: 0.2368 - val_loss: 1.7513 - val_accuracy: 0.2383\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7424 - accuracy: 0.2346 - val_loss: 1.7519 - val_accuracy: 0.2336\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7408 - accuracy: 0.2335 - val_loss: 1.7512 - val_accuracy: 0.2352\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7412 - accuracy: 0.2335 - val_loss: 1.7494 - val_accuracy: 0.2391\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7415 - accuracy: 0.2307 - val_loss: 1.7484 - val_accuracy: 0.2375\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7406 - accuracy: 0.2375 - val_loss: 1.7590 - val_accuracy: 0.2399\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7408 - accuracy: 0.2359 - val_loss: 1.7563 - val_accuracy: 0.2266\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7408 - accuracy: 0.2379 - val_loss: 1.7499 - val_accuracy: 0.2407\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7407 - accuracy: 0.2372 - val_loss: 1.7504 - val_accuracy: 0.2383\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7392 - accuracy: 0.2383 - val_loss: 1.7504 - val_accuracy: 0.2344\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7384 - accuracy: 0.2385 - val_loss: 1.7522 - val_accuracy: 0.2383\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7404 - accuracy: 0.2376 - val_loss: 1.7531 - val_accuracy: 0.2375\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7383 - accuracy: 0.2423 - val_loss: 1.7596 - val_accuracy: 0.2344\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7377 - accuracy: 0.2376 - val_loss: 1.7585 - val_accuracy: 0.2305\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7370 - accuracy: 0.2394 - val_loss: 1.7539 - val_accuracy: 0.2368\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7383 - accuracy: 0.2365 - val_loss: 1.7507 - val_accuracy: 0.2414\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7356 - accuracy: 0.2406 - val_loss: 1.7561 - val_accuracy: 0.2305\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7376 - accuracy: 0.2373 - val_loss: 1.7570 - val_accuracy: 0.2375\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7362 - accuracy: 0.2383 - val_loss: 1.7534 - val_accuracy: 0.2383\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7377 - accuracy: 0.2374 - val_loss: 1.7529 - val_accuracy: 0.2414\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7370 - accuracy: 0.2397 - val_loss: 1.7530 - val_accuracy: 0.2368\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7343 - accuracy: 0.2417 - val_loss: 1.7549 - val_accuracy: 0.2352\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7350 - accuracy: 0.2449 - val_loss: 1.7563 - val_accuracy: 0.2344\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffc3622ca0>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second training with learning_rate = 0.001\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "learning_rate = 0.001\n",
    "dense_model_jaccard = create_dense_model(X_train_meta_jaccard.shape[1], num_classes, learning_rate) # Create a new model with fresh weights and updated learning rate\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val), callbacks=[early_stopping, model_checkpoint_jaccard])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step\n",
      "Test set accuracy: 0.23\n",
      "Test set precision: 0.13\n",
      "Test set recall: 0.23\n",
      "Test set F1 score: 0.16\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model from the saved checkpoint\n",
    "best_model_jaccard = load_model('best_model_jaccard.h5')\n",
    "\n",
    "# Make predictions using the best model\n",
    "predictions = best_model_jaccard.predict(X_test_meta_jaccard)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the true labels of the test set in a variable called `Y_test`\n",
    "true_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Test set accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Calculate the precision (use `average` parameter to specify the type of averaging you want)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "print(f'Test set precision: {precision:.2f}')\n",
    "\n",
    "# Calculate the recall (use `average` parameter to specify the type of averaging you want)\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f'Test set recall: {recall:.2f}')\n",
    "\n",
    "# Calculate the F1 score (use `average` parameter to specify the type of averaging you want)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f'Test set F1 score: {f1:.2f}')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### INTIAL TESTING/TRAINING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6094 - accuracy: 0.2452 - val_loss: 1.6165 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6093 - accuracy: 0.2452 - val_loss: 1.6164 - val_accuracy: 0.2220\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6083 - accuracy: 0.2456 - val_loss: 1.6161 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2459 - val_loss: 1.6175 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6081 - accuracy: 0.2456 - val_loss: 1.6151 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6072 - accuracy: 0.2456 - val_loss: 1.6147 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6074 - accuracy: 0.2446 - val_loss: 1.6183 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6074 - accuracy: 0.2456 - val_loss: 1.6160 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.2455 - val_loss: 1.6177 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6073 - accuracy: 0.2456 - val_loss: 1.6139 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6066 - accuracy: 0.2456 - val_loss: 1.6147 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6069 - accuracy: 0.2459 - val_loss: 1.6150 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6063 - accuracy: 0.2456 - val_loss: 1.6153 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6066 - accuracy: 0.2456 - val_loss: 1.6157 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6063 - accuracy: 0.2456 - val_loss: 1.6142 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffb92f5700>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with callbacks for kmeans CATEGORICAL CROSS ENTROPY L2 NORM, CALLBACKS, LR=0.0005\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping, model_checkpoint_kmeans])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 11ms/step - loss: 2.3012 - accuracy: 0.2111 - val_loss: 1.8992 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.8012 - accuracy: 0.2226 - val_loss: 1.7282 - val_accuracy: 0.2220\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7021 - accuracy: 0.2260 - val_loss: 1.6783 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6644 - accuracy: 0.2264 - val_loss: 1.6545 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6500 - accuracy: 0.2305 - val_loss: 1.6444 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6376 - accuracy: 0.2348 - val_loss: 1.6385 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6333 - accuracy: 0.2373 - val_loss: 1.6352 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6293 - accuracy: 0.2377 - val_loss: 1.6344 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6269 - accuracy: 0.2390 - val_loss: 1.6295 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6233 - accuracy: 0.2409 - val_loss: 1.6306 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6219 - accuracy: 0.2444 - val_loss: 1.6282 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6199 - accuracy: 0.2452 - val_loss: 1.6279 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6185 - accuracy: 0.2455 - val_loss: 1.6242 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6176 - accuracy: 0.2448 - val_loss: 1.6233 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6158 - accuracy: 0.2431 - val_loss: 1.6229 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6152 - accuracy: 0.2456 - val_loss: 1.6212 - val_accuracy: 0.2220\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6139 - accuracy: 0.2457 - val_loss: 1.6237 - val_accuracy: 0.2220\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6138 - accuracy: 0.2458 - val_loss: 1.6225 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6130 - accuracy: 0.2446 - val_loss: 1.6191 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6118 - accuracy: 0.2454 - val_loss: 1.6195 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6120 - accuracy: 0.2445 - val_loss: 1.6191 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6114 - accuracy: 0.2458 - val_loss: 1.6212 - val_accuracy: 0.2220\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6108 - accuracy: 0.2456 - val_loss: 1.6206 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6106 - accuracy: 0.2456 - val_loss: 1.6171 - val_accuracy: 0.2220\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6102 - accuracy: 0.2456 - val_loss: 1.6166 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6100 - accuracy: 0.2456 - val_loss: 1.6164 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2393 - val_loss: 1.6161 - val_accuracy: 0.2220\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6094 - accuracy: 0.2448 - val_loss: 1.6212 - val_accuracy: 0.2220\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6093 - accuracy: 0.2454 - val_loss: 1.6173 - val_accuracy: 0.2220\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6085 - accuracy: 0.2455 - val_loss: 1.6166 - val_accuracy: 0.2220\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6081 - accuracy: 0.2452 - val_loss: 1.6171 - val_accuracy: 0.2220\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6080 - accuracy: 0.2449 - val_loss: 1.6177 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffb6e31eb0>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with callbacks for kmeans CATEGORICAL CROSS ENTROPY L2 NORM, CALLBACKS, LR=0.001\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping, model_checkpoint_kmeans])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 9ms/step - loss: 2.2855 - accuracy: 0.2157 - val_loss: 1.8952 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.7936 - accuracy: 0.2232 - val_loss: 1.7168 - val_accuracy: 0.2196\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6960 - accuracy: 0.2301 - val_loss: 1.6715 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6623 - accuracy: 0.2360 - val_loss: 1.6491 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6482 - accuracy: 0.2393 - val_loss: 1.6459 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6396 - accuracy: 0.2369 - val_loss: 1.6366 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6324 - accuracy: 0.2430 - val_loss: 1.6351 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6304 - accuracy: 0.2437 - val_loss: 1.6323 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6271 - accuracy: 0.2450 - val_loss: 1.6281 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6242 - accuracy: 0.2445 - val_loss: 1.6308 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6226 - accuracy: 0.2425 - val_loss: 1.6272 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6212 - accuracy: 0.2456 - val_loss: 1.6253 - val_accuracy: 0.2391\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6203 - accuracy: 0.2437 - val_loss: 1.6232 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6178 - accuracy: 0.2441 - val_loss: 1.6261 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6176 - accuracy: 0.2439 - val_loss: 1.6218 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6158 - accuracy: 0.2439 - val_loss: 1.6200 - val_accuracy: 0.2220\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6165 - accuracy: 0.2442 - val_loss: 1.6227 - val_accuracy: 0.2220\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6148 - accuracy: 0.2473 - val_loss: 1.6205 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6147 - accuracy: 0.2468 - val_loss: 1.6249 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6141 - accuracy: 0.2433 - val_loss: 1.6183 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6125 - accuracy: 0.2450 - val_loss: 1.6201 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6139 - accuracy: 0.2432 - val_loss: 1.6218 - val_accuracy: 0.2438\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6123 - accuracy: 0.2428 - val_loss: 1.6180 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6113 - accuracy: 0.2470 - val_loss: 1.6159 - val_accuracy: 0.2220\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6113 - accuracy: 0.2476 - val_loss: 1.6194 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6115 - accuracy: 0.2467 - val_loss: 1.6159 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6123 - accuracy: 0.2460 - val_loss: 1.6175 - val_accuracy: 0.2220\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6100 - accuracy: 0.2470 - val_loss: 1.6167 - val_accuracy: 0.2220\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 1.6114 - accuracy: 0.2452 - val_loss: 1.6146 - val_accuracy: 0.2212\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6104 - accuracy: 0.2427 - val_loss: 1.6161 - val_accuracy: 0.2220\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6109 - accuracy: 0.2420 - val_loss: 1.6168 - val_accuracy: 0.2220\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6087 - accuracy: 0.2491 - val_loss: 1.6167 - val_accuracy: 0.2220\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6090 - accuracy: 0.2473 - val_loss: 1.6149 - val_accuracy: 0.2220\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 1.6083 - accuracy: 0.2459 - val_loss: 1.6157 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffb69cfbe0>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with callbacks for jaccard CATEGORICAL CROSS ENTROPY L2 NORM, CALLBACKS, LR=0.001\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val), callbacks=[early_stopping, model_checkpoint_jaccard])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 2s 8ms/step - loss: 1.0149 - accuracy: 0.2062 - val_loss: 0.5886 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5330 - accuracy: 0.2193 - val_loss: 0.4711 - val_accuracy: 0.2220\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.2127 - val_loss: 0.4493 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4540 - accuracy: 0.2190 - val_loss: 0.4405 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4442 - accuracy: 0.2246 - val_loss: 0.4356 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4399 - accuracy: 0.2294 - val_loss: 0.4326 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4360 - accuracy: 0.2346 - val_loss: 0.4308 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4343 - accuracy: 0.2336 - val_loss: 0.4298 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.2323 - val_loss: 0.4285 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.2355 - val_loss: 0.4276 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.2345 - val_loss: 0.4269 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.2400 - val_loss: 0.4253 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4277 - accuracy: 0.2417 - val_loss: 0.4246 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4264 - accuracy: 0.2387 - val_loss: 0.4246 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.2408 - val_loss: 0.4242 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4255 - accuracy: 0.2416 - val_loss: 0.4231 - val_accuracy: 0.2220\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4245 - accuracy: 0.2443 - val_loss: 0.4227 - val_accuracy: 0.2220\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4244 - accuracy: 0.2444 - val_loss: 0.4226 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4239 - accuracy: 0.2432 - val_loss: 0.4227 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4238 - accuracy: 0.2456 - val_loss: 0.4226 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.2451 - val_loss: 0.4223 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.2452 - val_loss: 0.4217 - val_accuracy: 0.2220\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.2458 - val_loss: 0.4216 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.2456 - val_loss: 0.4215 - val_accuracy: 0.2220\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4222 - accuracy: 0.2452 - val_loss: 0.4209 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4220 - accuracy: 0.2441 - val_loss: 0.4217 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4218 - accuracy: 0.2455 - val_loss: 0.4207 - val_accuracy: 0.2220\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.2450 - val_loss: 0.4204 - val_accuracy: 0.2220\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4215 - accuracy: 0.2423 - val_loss: 0.4211 - val_accuracy: 0.2220\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.2452 - val_loss: 0.4205 - val_accuracy: 0.2220\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.2441 - val_loss: 0.4202 - val_accuracy: 0.2220\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.2436 - val_loss: 0.4211 - val_accuracy: 0.2220\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.2458 - val_loss: 0.4206 - val_accuracy: 0.2220\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.2450 - val_loss: 0.4204 - val_accuracy: 0.2220\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.2456 - val_loss: 0.4201 - val_accuracy: 0.2220\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4205 - accuracy: 0.2465 - val_loss: 0.4204 - val_accuracy: 0.2220\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.2441 - val_loss: 0.4196 - val_accuracy: 0.2220\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.2443 - val_loss: 0.4208 - val_accuracy: 0.2220\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.2452 - val_loss: 0.4208 - val_accuracy: 0.2220\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.2393 - val_loss: 0.4211 - val_accuracy: 0.2220\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.2442 - val_loss: 0.4196 - val_accuracy: 0.2220\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4199 - accuracy: 0.2445 - val_loss: 0.4205 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffb1e60e20>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with callbacks for jaccard\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val), callbacks=[early_stopping, model_checkpoint_jaccard])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.2454 - val_loss: 0.4214 - val_accuracy: 0.2220\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.2462 - val_loss: 0.4219 - val_accuracy: 0.2220\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4213 - accuracy: 0.2448 - val_loss: 0.4202 - val_accuracy: 0.2220\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.2453 - val_loss: 0.4208 - val_accuracy: 0.2220\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.2450 - val_loss: 0.4207 - val_accuracy: 0.2220\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4206 - accuracy: 0.2434 - val_loss: 0.4206 - val_accuracy: 0.2220\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.2456 - val_loss: 0.4205 - val_accuracy: 0.2220\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4205 - accuracy: 0.2450 - val_loss: 0.4197 - val_accuracy: 0.2220\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.2454 - val_loss: 0.4201 - val_accuracy: 0.2220\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.2456 - val_loss: 0.4206 - val_accuracy: 0.2220\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.2442 - val_loss: 0.4204 - val_accuracy: 0.2220\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.2452 - val_loss: 0.4199 - val_accuracy: 0.2220\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.2457 - val_loss: 0.4195 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.2458 - val_loss: 0.4196 - val_accuracy: 0.2220\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4196 - accuracy: 0.2445 - val_loss: 0.4194 - val_accuracy: 0.2220\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4196 - accuracy: 0.2442 - val_loss: 0.4198 - val_accuracy: 0.2220\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4194 - accuracy: 0.2418 - val_loss: 0.4197 - val_accuracy: 0.2220\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4194 - accuracy: 0.2438 - val_loss: 0.4199 - val_accuracy: 0.2220\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4193 - accuracy: 0.2445 - val_loss: 0.4201 - val_accuracy: 0.2220\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.2453 - val_loss: 0.4193 - val_accuracy: 0.2220\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.2440 - val_loss: 0.4193 - val_accuracy: 0.2220\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.2464 - val_loss: 0.4197 - val_accuracy: 0.2220\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.2457 - val_loss: 0.4192 - val_accuracy: 0.2220\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4190 - accuracy: 0.2454 - val_loss: 0.4192 - val_accuracy: 0.2220\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4190 - accuracy: 0.2462 - val_loss: 0.4194 - val_accuracy: 0.2220\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4186 - accuracy: 0.2449 - val_loss: 0.4198 - val_accuracy: 0.2220\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4186 - accuracy: 0.2425 - val_loss: 0.4199 - val_accuracy: 0.2220\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4185 - accuracy: 0.2417 - val_loss: 0.4192 - val_accuracy: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffb5958760>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train mdoel with callbacks for kmeans\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val), callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the K-means model YES L2 NORM DEFAULT LR\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdense_model_kmeans\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_meta_kmeans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_val_meta_kmeans\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fyp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:956\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    954\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    955\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[1;32m--> 956\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    957\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    958\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m    960\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    961\u001B[0m   \u001B[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "# Train the K-means model YES L2 NORM DEFAULT LR\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=30, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.3753 - val_loss: 1.3347 - val_accuracy: 0.1893\n",
      "Epoch 2/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3714 - accuracy: 0.3753 - val_loss: 1.4017 - val_accuracy: 0.1931\n",
      "Epoch 3/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3710 - accuracy: 0.3805 - val_loss: 1.2948 - val_accuracy: 0.2017\n",
      "Epoch 4/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.3789 - val_loss: 1.3937 - val_accuracy: 0.1893\n",
      "Epoch 5/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.3853 - val_loss: 1.3730 - val_accuracy: 0.1963\n",
      "Epoch 6/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.3896 - val_loss: 1.5697 - val_accuracy: 0.1955\n",
      "Epoch 7/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.3884 - val_loss: 1.7194 - val_accuracy: 0.1916\n",
      "Epoch 8/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3664 - accuracy: 0.3872 - val_loss: 1.5743 - val_accuracy: 0.2064\n",
      "Epoch 9/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.3884 - val_loss: 1.7176 - val_accuracy: 0.1955\n",
      "Epoch 10/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.3891 - val_loss: 1.6818 - val_accuracy: 0.1846\n",
      "Epoch 11/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.3879 - val_loss: 1.7821 - val_accuracy: 0.1861\n",
      "Epoch 12/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.3888 - val_loss: 1.7652 - val_accuracy: 0.2033\n",
      "Epoch 13/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.3912 - val_loss: 2.0956 - val_accuracy: 0.1939\n",
      "Epoch 14/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3658 - accuracy: 0.3886 - val_loss: 1.7900 - val_accuracy: 0.2009\n",
      "Epoch 15/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.3878 - val_loss: 1.8506 - val_accuracy: 0.1939\n",
      "Epoch 16/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.3870 - val_loss: 1.7403 - val_accuracy: 0.1947\n",
      "Epoch 17/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.3897 - val_loss: 1.7290 - val_accuracy: 0.1900\n",
      "Epoch 18/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.3888 - val_loss: 1.6888 - val_accuracy: 0.1986\n",
      "Epoch 19/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.3872 - val_loss: 1.6328 - val_accuracy: 0.2017\n",
      "Epoch 20/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.3937 - val_loss: 2.0938 - val_accuracy: 0.1986\n",
      "Epoch 21/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.3930 - val_loss: 1.8761 - val_accuracy: 0.1900\n",
      "Epoch 22/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3651 - accuracy: 0.3896 - val_loss: 1.9153 - val_accuracy: 0.2009\n",
      "Epoch 23/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3652 - accuracy: 0.3910 - val_loss: 1.9699 - val_accuracy: 0.1986\n",
      "Epoch 24/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3636 - accuracy: 0.3978 - val_loss: 1.8897 - val_accuracy: 0.1955\n",
      "Epoch 25/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3643 - accuracy: 0.3948 - val_loss: 1.9704 - val_accuracy: 0.1931\n",
      "Epoch 26/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3628 - accuracy: 0.3916 - val_loss: 1.9201 - val_accuracy: 0.2002\n",
      "Epoch 27/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.3916 - val_loss: 1.7903 - val_accuracy: 0.1970\n",
      "Epoch 28/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3622 - accuracy: 0.3932 - val_loss: 2.2303 - val_accuracy: 0.2072\n",
      "Epoch 29/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3628 - accuracy: 0.3963 - val_loss: 2.0861 - val_accuracy: 0.1986\n",
      "Epoch 30/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.3972 - val_loss: 2.1484 - val_accuracy: 0.2095\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffa1d6a3d0>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the K-means model YES L2 NORM DEFAULT LR\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=30, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.4371 - val_loss: 1.2994 - val_accuracy: 0.2321\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.4327 - val_loss: 1.2738 - val_accuracy: 0.2360\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.4328 - val_loss: 1.3376 - val_accuracy: 0.2227\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.4313 - val_loss: 1.4110 - val_accuracy: 0.2181\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3261 - accuracy: 0.4359 - val_loss: 1.4133 - val_accuracy: 0.2383\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.4296 - val_loss: 1.3576 - val_accuracy: 0.2321\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.4329 - val_loss: 1.4195 - val_accuracy: 0.2290\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3264 - accuracy: 0.4304 - val_loss: 1.4544 - val_accuracy: 0.2321\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.4333 - val_loss: 1.3835 - val_accuracy: 0.2305\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.4330 - val_loss: 1.3779 - val_accuracy: 0.2274\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.4308 - val_loss: 1.3816 - val_accuracy: 0.2235\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.4373 - val_loss: 1.4967 - val_accuracy: 0.2375\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.4247 - val_loss: 1.3795 - val_accuracy: 0.2383\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.4361 - val_loss: 1.4490 - val_accuracy: 0.2407\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.4271 - val_loss: 1.3395 - val_accuracy: 0.2344\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.4316 - val_loss: 1.6043 - val_accuracy: 0.2251\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3256 - accuracy: 0.4349 - val_loss: 1.4611 - val_accuracy: 0.2321\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3251 - accuracy: 0.4310 - val_loss: 1.5564 - val_accuracy: 0.2321\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.4300 - val_loss: 1.5569 - val_accuracy: 0.2321\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.4320 - val_loss: 1.5429 - val_accuracy: 0.2329\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.4406 - val_loss: 1.4729 - val_accuracy: 0.2298\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3275 - accuracy: 0.4394 - val_loss: 1.2791 - val_accuracy: 0.2298\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3304 - accuracy: 0.4325 - val_loss: 1.3134 - val_accuracy: 0.2360\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.4314 - val_loss: 1.3664 - val_accuracy: 0.2383\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3239 - accuracy: 0.4358 - val_loss: 1.4619 - val_accuracy: 0.2375\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.4360 - val_loss: 1.4677 - val_accuracy: 0.2391\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.4357 - val_loss: 1.3899 - val_accuracy: 0.2352\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3268 - accuracy: 0.4407 - val_loss: 1.4113 - val_accuracy: 0.2414\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.4361 - val_loss: 1.4014 - val_accuracy: 0.2313\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.4370 - val_loss: 1.3724 - val_accuracy: 0.2399\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3240 - accuracy: 0.4375 - val_loss: 1.5819 - val_accuracy: 0.2329\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.4382 - val_loss: 1.4675 - val_accuracy: 0.2368\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.4350 - val_loss: 1.5303 - val_accuracy: 0.2399\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.4333 - val_loss: 1.3719 - val_accuracy: 0.2391\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3241 - accuracy: 0.4344 - val_loss: 1.4621 - val_accuracy: 0.2407\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.4356 - val_loss: 1.3858 - val_accuracy: 0.2453\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.4369 - val_loss: 1.4683 - val_accuracy: 0.2383\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.4359 - val_loss: 1.6474 - val_accuracy: 0.2368\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3254 - accuracy: 0.4363 - val_loss: 1.3083 - val_accuracy: 0.2383\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3245 - accuracy: 0.4377 - val_loss: 1.3344 - val_accuracy: 0.2329\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.4360 - val_loss: 1.4517 - val_accuracy: 0.2360\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.4411 - val_loss: 1.4251 - val_accuracy: 0.2282\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3237 - accuracy: 0.4354 - val_loss: 1.6524 - val_accuracy: 0.2461\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3252 - accuracy: 0.4369 - val_loss: 1.4599 - val_accuracy: 0.2336\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3236 - accuracy: 0.4393 - val_loss: 1.4455 - val_accuracy: 0.2360\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.4372 - val_loss: 1.3841 - val_accuracy: 0.2422\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.4412 - val_loss: 1.6129 - val_accuracy: 0.2422\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3243 - accuracy: 0.4393 - val_loss: 1.3865 - val_accuracy: 0.2383\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.4389 - val_loss: 1.4891 - val_accuracy: 0.2399\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3240 - accuracy: 0.4406 - val_loss: 1.2676 - val_accuracy: 0.2313\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.4324 - val_loss: 1.3779 - val_accuracy: 0.2329\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3239 - accuracy: 0.4336 - val_loss: 1.4618 - val_accuracy: 0.2352\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.4370 - val_loss: 1.4014 - val_accuracy: 0.2430\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.4354 - val_loss: 1.3120 - val_accuracy: 0.2321\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.4337 - val_loss: 1.4312 - val_accuracy: 0.2407\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.4393 - val_loss: 1.3739 - val_accuracy: 0.2438\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.4361 - val_loss: 1.5061 - val_accuracy: 0.2391\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.4402 - val_loss: 1.4151 - val_accuracy: 0.2407\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.4385 - val_loss: 1.5002 - val_accuracy: 0.2375\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3229 - accuracy: 0.4373 - val_loss: 1.6101 - val_accuracy: 0.2445\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.4392 - val_loss: 1.2704 - val_accuracy: 0.2344\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3244 - accuracy: 0.4381 - val_loss: 1.4587 - val_accuracy: 0.2375\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.4333 - val_loss: 1.2517 - val_accuracy: 0.2477\n",
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3270 - accuracy: 0.4310 - val_loss: 1.2576 - val_accuracy: 0.2391\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.4337 - val_loss: 1.3142 - val_accuracy: 0.2344\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3236 - accuracy: 0.4408 - val_loss: 1.4686 - val_accuracy: 0.2368\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.4330 - val_loss: 1.4717 - val_accuracy: 0.2321\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.4323 - val_loss: 1.2218 - val_accuracy: 0.2500\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3236 - accuracy: 0.4371 - val_loss: 1.3117 - val_accuracy: 0.2336\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3225 - accuracy: 0.4407 - val_loss: 1.4855 - val_accuracy: 0.2422\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.4425 - val_loss: 1.4849 - val_accuracy: 0.2422\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.4462 - val_loss: 1.5742 - val_accuracy: 0.2469\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.4402 - val_loss: 1.4849 - val_accuracy: 0.2399\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.4401 - val_loss: 1.4873 - val_accuracy: 0.2368\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.4455 - val_loss: 1.5197 - val_accuracy: 0.2407\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.4470 - val_loss: 1.6020 - val_accuracy: 0.2329\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3206 - accuracy: 0.4430 - val_loss: 1.6048 - val_accuracy: 0.2430\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.4341 - val_loss: 1.6011 - val_accuracy: 0.2305\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.4431 - val_loss: 1.6073 - val_accuracy: 0.2352\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3218 - accuracy: 0.4397 - val_loss: 1.7113 - val_accuracy: 0.2305\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.4409 - val_loss: 1.4900 - val_accuracy: 0.2313\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.4445 - val_loss: 1.5715 - val_accuracy: 0.2321\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.4420 - val_loss: 1.6247 - val_accuracy: 0.2259\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.4429 - val_loss: 1.6971 - val_accuracy: 0.2282\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.4397 - val_loss: 1.6306 - val_accuracy: 0.2375\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.4437 - val_loss: 1.6356 - val_accuracy: 0.2445\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.4443 - val_loss: 1.5562 - val_accuracy: 0.2344\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.4386 - val_loss: 1.5952 - val_accuracy: 0.2469\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.4400 - val_loss: 1.4143 - val_accuracy: 0.2352\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3206 - accuracy: 0.4448 - val_loss: 1.6296 - val_accuracy: 0.2344\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.4454 - val_loss: 1.5036 - val_accuracy: 0.2360\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.4409 - val_loss: 1.4318 - val_accuracy: 0.2391\n",
      "Epoch 93/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.4359 - val_loss: 1.6272 - val_accuracy: 0.2329\n",
      "Epoch 94/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3218 - accuracy: 0.4379 - val_loss: 1.4288 - val_accuracy: 0.2352\n",
      "Epoch 95/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3207 - accuracy: 0.4433 - val_loss: 1.6172 - val_accuracy: 0.2368\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.4444 - val_loss: 1.6490 - val_accuracy: 0.2305\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.4341 - val_loss: 1.8437 - val_accuracy: 0.2313\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.4392 - val_loss: 1.5457 - val_accuracy: 0.2391\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.4434 - val_loss: 1.6088 - val_accuracy: 0.2313\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.4419 - val_loss: 1.7001 - val_accuracy: 0.2430\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.4395 - val_loss: 1.5911 - val_accuracy: 0.2469\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.4412 - val_loss: 1.7237 - val_accuracy: 0.2391\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.4392 - val_loss: 1.7276 - val_accuracy: 0.2445\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3200 - accuracy: 0.4408 - val_loss: 1.7042 - val_accuracy: 0.2508\n",
      "Epoch 105/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.4423 - val_loss: 1.7197 - val_accuracy: 0.2383\n",
      "Epoch 106/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3228 - accuracy: 0.4423 - val_loss: 1.5179 - val_accuracy: 0.2375\n",
      "Epoch 107/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3210 - accuracy: 0.4440 - val_loss: 1.6275 - val_accuracy: 0.2360\n",
      "Epoch 108/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3202 - accuracy: 0.4406 - val_loss: 1.6567 - val_accuracy: 0.2321\n",
      "Epoch 109/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3200 - accuracy: 0.4475 - val_loss: 1.6510 - val_accuracy: 0.2313\n",
      "Epoch 110/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.4450 - val_loss: 1.7513 - val_accuracy: 0.2360\n",
      "Epoch 111/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.4432 - val_loss: 1.6810 - val_accuracy: 0.2368\n",
      "Epoch 112/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.4404 - val_loss: 1.6423 - val_accuracy: 0.2407\n",
      "Epoch 113/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.4443 - val_loss: 1.5428 - val_accuracy: 0.2375\n",
      "Epoch 114/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.4422 - val_loss: 1.6738 - val_accuracy: 0.2375\n",
      "Epoch 115/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.4453 - val_loss: 1.7551 - val_accuracy: 0.2336\n",
      "Epoch 116/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.4370 - val_loss: 1.7331 - val_accuracy: 0.2422\n",
      "Epoch 117/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.4461 - val_loss: 1.8226 - val_accuracy: 0.2368\n",
      "Epoch 118/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.4435 - val_loss: 1.8336 - val_accuracy: 0.2391\n",
      "Epoch 119/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.4426 - val_loss: 1.5191 - val_accuracy: 0.2430\n",
      "Epoch 120/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 0.4400 - val_loss: 1.2738 - val_accuracy: 0.2336\n",
      "Epoch 121/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3232 - accuracy: 0.4362 - val_loss: 1.3976 - val_accuracy: 0.2414\n",
      "Epoch 122/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.4441 - val_loss: 1.6179 - val_accuracy: 0.2407\n",
      "Epoch 123/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.4408 - val_loss: 1.7901 - val_accuracy: 0.2375\n",
      "Epoch 124/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.4413 - val_loss: 1.6993 - val_accuracy: 0.2484\n",
      "Epoch 125/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.4453 - val_loss: 1.4983 - val_accuracy: 0.2445\n",
      "Epoch 126/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.4451 - val_loss: 1.7803 - val_accuracy: 0.2445\n",
      "Epoch 127/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.4414 - val_loss: 1.7907 - val_accuracy: 0.2352\n",
      "Epoch 128/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.4443 - val_loss: 1.8974 - val_accuracy: 0.2375\n",
      "Epoch 129/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.4429 - val_loss: 1.8632 - val_accuracy: 0.2414\n",
      "Epoch 130/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3192 - accuracy: 0.4453 - val_loss: 2.0801 - val_accuracy: 0.2399\n",
      "Epoch 131/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3200 - accuracy: 0.4496 - val_loss: 1.7803 - val_accuracy: 0.2352\n",
      "Epoch 132/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.4431 - val_loss: 1.9002 - val_accuracy: 0.2383\n",
      "Epoch 133/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.4425 - val_loss: 1.7686 - val_accuracy: 0.2438\n",
      "Epoch 134/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.4442 - val_loss: 1.8387 - val_accuracy: 0.2391\n",
      "Epoch 135/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.4492 - val_loss: 1.8253 - val_accuracy: 0.2407\n",
      "Epoch 136/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.4479 - val_loss: 1.8235 - val_accuracy: 0.2375\n",
      "Epoch 137/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.4404 - val_loss: 1.5297 - val_accuracy: 0.2305\n",
      "Epoch 138/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.4424 - val_loss: 1.7083 - val_accuracy: 0.2375\n",
      "Epoch 139/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.4429 - val_loss: 1.6242 - val_accuracy: 0.2399\n",
      "Epoch 140/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3254 - accuracy: 0.4410 - val_loss: 1.2555 - val_accuracy: 0.2430\n",
      "Epoch 141/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.4435 - val_loss: 1.3901 - val_accuracy: 0.2313\n",
      "Epoch 142/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.4416 - val_loss: 1.4152 - val_accuracy: 0.2220\n",
      "Epoch 143/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3216 - accuracy: 0.4464 - val_loss: 1.3962 - val_accuracy: 0.2336\n",
      "Epoch 144/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.4421 - val_loss: 1.4593 - val_accuracy: 0.2383\n",
      "Epoch 145/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.4403 - val_loss: 1.4855 - val_accuracy: 0.2375\n",
      "Epoch 146/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.4439 - val_loss: 1.6039 - val_accuracy: 0.2321\n",
      "Epoch 147/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.4436 - val_loss: 1.6406 - val_accuracy: 0.2321\n",
      "Epoch 148/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3225 - accuracy: 0.4418 - val_loss: 1.5546 - val_accuracy: 0.2290\n",
      "Epoch 149/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.4412 - val_loss: 1.3951 - val_accuracy: 0.2305\n",
      "Epoch 150/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3191 - accuracy: 0.4427 - val_loss: 1.5246 - val_accuracy: 0.2407\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffa6971310>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Jaccard model YES L2 NORM DEFAULT LR\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.3421 - val_loss: 0.5322 - val_accuracy: 0.1947\n",
      "Epoch 2/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.3451 - val_loss: 0.5539 - val_accuracy: 0.1994\n",
      "Epoch 3/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.3420 - val_loss: 0.5450 - val_accuracy: 0.1970\n",
      "Epoch 4/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3857 - accuracy: 0.3580 - val_loss: 0.5517 - val_accuracy: 0.1931\n",
      "Epoch 5/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3849 - accuracy: 0.3540 - val_loss: 0.6257 - val_accuracy: 0.1924\n",
      "Epoch 6/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3845 - accuracy: 0.3579 - val_loss: 0.6276 - val_accuracy: 0.1916\n",
      "Epoch 7/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3834 - accuracy: 0.3566 - val_loss: 0.6909 - val_accuracy: 0.1986\n",
      "Epoch 8/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3832 - accuracy: 0.3532 - val_loss: 0.6801 - val_accuracy: 0.1939\n",
      "Epoch 9/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3819 - accuracy: 0.3602 - val_loss: 0.7731 - val_accuracy: 0.1947\n",
      "Epoch 10/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3821 - accuracy: 0.3578 - val_loss: 0.7274 - val_accuracy: 0.2009\n",
      "Epoch 11/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3801 - accuracy: 0.3593 - val_loss: 0.8002 - val_accuracy: 0.2002\n",
      "Epoch 12/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.3646 - val_loss: 0.8747 - val_accuracy: 0.1799\n",
      "Epoch 13/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3800 - accuracy: 0.3619 - val_loss: 0.7711 - val_accuracy: 0.1986\n",
      "Epoch 14/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.3647 - val_loss: 0.8443 - val_accuracy: 0.1916\n",
      "Epoch 15/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3779 - accuracy: 0.3644 - val_loss: 1.0416 - val_accuracy: 0.1931\n",
      "Epoch 16/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3761 - accuracy: 0.3669 - val_loss: 0.9405 - val_accuracy: 0.1916\n",
      "Epoch 17/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.3695 - val_loss: 0.9682 - val_accuracy: 0.1916\n",
      "Epoch 18/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3749 - accuracy: 0.3690 - val_loss: 1.0282 - val_accuracy: 0.1994\n",
      "Epoch 19/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.3653 - val_loss: 1.1186 - val_accuracy: 0.1916\n",
      "Epoch 20/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.3710 - val_loss: 1.0967 - val_accuracy: 0.1861\n",
      "Epoch 21/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.3710 - val_loss: 1.2986 - val_accuracy: 0.1893\n",
      "Epoch 22/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3746 - accuracy: 0.3698 - val_loss: 1.2480 - val_accuracy: 0.1900\n",
      "Epoch 23/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.3759 - val_loss: 1.3316 - val_accuracy: 0.1908\n",
      "Epoch 24/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3730 - accuracy: 0.3751 - val_loss: 1.1883 - val_accuracy: 0.1807\n",
      "Epoch 25/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3715 - accuracy: 0.3744 - val_loss: 1.4402 - val_accuracy: 0.1900\n",
      "Epoch 26/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.3802 - val_loss: 1.3813 - val_accuracy: 0.1955\n",
      "Epoch 27/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3700 - accuracy: 0.3789 - val_loss: 1.3843 - val_accuracy: 0.1869\n",
      "Epoch 28/30\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3699 - accuracy: 0.3813 - val_loss: 1.4314 - val_accuracy: 0.1924\n",
      "Epoch 29/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3702 - accuracy: 0.3777 - val_loss: 1.6255 - val_accuracy: 0.1846\n",
      "Epoch 30/30\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3712 - accuracy: 0.3774 - val_loss: 1.3768 - val_accuracy: 0.1931\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffaaeccbb0>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the K-means model NO L2 NORM DEFAULT LR\n",
    "dense_model_kmeans.fit(X_train_meta_kmeans, Y_train, epochs=30, batch_size=128, validation_data=(X_val_meta_kmeans, Y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.4070 - val_loss: 0.6273 - val_accuracy: 0.2399\n",
      "Epoch 2/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3518 - accuracy: 0.4033 - val_loss: 0.6363 - val_accuracy: 0.2329\n",
      "Epoch 3/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3504 - accuracy: 0.4065 - val_loss: 0.6416 - val_accuracy: 0.2305\n",
      "Epoch 4/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3494 - accuracy: 0.4097 - val_loss: 0.6747 - val_accuracy: 0.2422\n",
      "Epoch 5/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.4093 - val_loss: 0.7185 - val_accuracy: 0.2368\n",
      "Epoch 6/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3492 - accuracy: 0.4109 - val_loss: 0.6636 - val_accuracy: 0.2539\n",
      "Epoch 7/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3504 - accuracy: 0.4065 - val_loss: 0.6437 - val_accuracy: 0.2516\n",
      "Epoch 8/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3518 - accuracy: 0.4047 - val_loss: 0.6282 - val_accuracy: 0.2204\n",
      "Epoch 9/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3487 - accuracy: 0.4106 - val_loss: 0.7040 - val_accuracy: 0.2453\n",
      "Epoch 10/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.4107 - val_loss: 0.6458 - val_accuracy: 0.2430\n",
      "Epoch 11/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3513 - accuracy: 0.4070 - val_loss: 0.6147 - val_accuracy: 0.2414\n",
      "Epoch 12/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3509 - accuracy: 0.4119 - val_loss: 0.6967 - val_accuracy: 0.2298\n",
      "Epoch 13/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.4107 - val_loss: 0.6711 - val_accuracy: 0.2220\n",
      "Epoch 14/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.4105 - val_loss: 0.7275 - val_accuracy: 0.2305\n",
      "Epoch 15/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.4110 - val_loss: 0.7125 - val_accuracy: 0.2305\n",
      "Epoch 16/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3512 - accuracy: 0.4125 - val_loss: 0.6455 - val_accuracy: 0.2368\n",
      "Epoch 17/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.4095 - val_loss: 0.7057 - val_accuracy: 0.2399\n",
      "Epoch 18/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.4137 - val_loss: 0.7167 - val_accuracy: 0.2383\n",
      "Epoch 19/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.4095 - val_loss: 0.6619 - val_accuracy: 0.2391\n",
      "Epoch 20/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3474 - accuracy: 0.4111 - val_loss: 0.7226 - val_accuracy: 0.2399\n",
      "Epoch 21/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3454 - accuracy: 0.4136 - val_loss: 0.7014 - val_accuracy: 0.2375\n",
      "Epoch 22/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3455 - accuracy: 0.4097 - val_loss: 0.7659 - val_accuracy: 0.2352\n",
      "Epoch 23/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3471 - accuracy: 0.4162 - val_loss: 0.6916 - val_accuracy: 0.2352\n",
      "Epoch 24/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.4196 - val_loss: 0.6766 - val_accuracy: 0.2329\n",
      "Epoch 25/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.4145 - val_loss: 0.6711 - val_accuracy: 0.2430\n",
      "Epoch 26/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.4206 - val_loss: 0.6432 - val_accuracy: 0.2438\n",
      "Epoch 27/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.4171 - val_loss: 0.7172 - val_accuracy: 0.2500\n",
      "Epoch 28/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.4100 - val_loss: 0.7726 - val_accuracy: 0.2375\n",
      "Epoch 29/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3439 - accuracy: 0.4127 - val_loss: 0.7681 - val_accuracy: 0.2469\n",
      "Epoch 30/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.4133 - val_loss: 0.7601 - val_accuracy: 0.2453\n",
      "Epoch 31/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3429 - accuracy: 0.4146 - val_loss: 0.8053 - val_accuracy: 0.2375\n",
      "Epoch 32/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.4184 - val_loss: 0.8293 - val_accuracy: 0.2445\n",
      "Epoch 33/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.4210 - val_loss: 0.7872 - val_accuracy: 0.2422\n",
      "Epoch 34/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.4170 - val_loss: 0.7643 - val_accuracy: 0.2352\n",
      "Epoch 35/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3433 - accuracy: 0.4152 - val_loss: 0.8444 - val_accuracy: 0.2469\n",
      "Epoch 36/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.4157 - val_loss: 0.8458 - val_accuracy: 0.2399\n",
      "Epoch 37/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.4214 - val_loss: 0.9083 - val_accuracy: 0.2368\n",
      "Epoch 38/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.4176 - val_loss: 0.9047 - val_accuracy: 0.2391\n",
      "Epoch 39/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.4213 - val_loss: 0.8119 - val_accuracy: 0.2391\n",
      "Epoch 40/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.4159 - val_loss: 0.7591 - val_accuracy: 0.2298\n",
      "Epoch 41/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.4178 - val_loss: 0.7499 - val_accuracy: 0.2344\n",
      "Epoch 42/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3519 - accuracy: 0.4124 - val_loss: 0.6803 - val_accuracy: 0.2368\n",
      "Epoch 43/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3468 - accuracy: 0.4146 - val_loss: 0.7378 - val_accuracy: 0.2360\n",
      "Epoch 44/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3457 - accuracy: 0.4176 - val_loss: 0.7504 - val_accuracy: 0.2399\n",
      "Epoch 45/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3460 - accuracy: 0.4164 - val_loss: 0.7226 - val_accuracy: 0.2430\n",
      "Epoch 46/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3422 - accuracy: 0.4203 - val_loss: 0.7263 - val_accuracy: 0.2399\n",
      "Epoch 47/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3434 - accuracy: 0.4176 - val_loss: 0.7413 - val_accuracy: 0.2407\n",
      "Epoch 48/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.4217 - val_loss: 0.8384 - val_accuracy: 0.2399\n",
      "Epoch 49/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3409 - accuracy: 0.4181 - val_loss: 0.8113 - val_accuracy: 0.2282\n",
      "Epoch 50/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3400 - accuracy: 0.4202 - val_loss: 0.8526 - val_accuracy: 0.2414\n",
      "Epoch 51/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3399 - accuracy: 0.4164 - val_loss: 0.8929 - val_accuracy: 0.2305\n",
      "Epoch 52/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3396 - accuracy: 0.4179 - val_loss: 0.9323 - val_accuracy: 0.2344\n",
      "Epoch 53/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.4180 - val_loss: 0.8616 - val_accuracy: 0.2375\n",
      "Epoch 54/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.4212 - val_loss: 0.8988 - val_accuracy: 0.2298\n",
      "Epoch 55/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.4214 - val_loss: 0.8972 - val_accuracy: 0.2438\n",
      "Epoch 56/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3380 - accuracy: 0.4243 - val_loss: 1.0020 - val_accuracy: 0.2383\n",
      "Epoch 57/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3392 - accuracy: 0.4231 - val_loss: 1.0017 - val_accuracy: 0.2368\n",
      "Epoch 58/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3394 - accuracy: 0.4223 - val_loss: 0.9456 - val_accuracy: 0.2360\n",
      "Epoch 59/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3380 - accuracy: 0.4254 - val_loss: 0.9629 - val_accuracy: 0.2227\n",
      "Epoch 60/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3383 - accuracy: 0.4240 - val_loss: 1.0241 - val_accuracy: 0.2391\n",
      "Epoch 61/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.4221 - val_loss: 0.9442 - val_accuracy: 0.2391\n",
      "Epoch 62/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3415 - accuracy: 0.4170 - val_loss: 0.8936 - val_accuracy: 0.2336\n",
      "Epoch 63/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3413 - accuracy: 0.4198 - val_loss: 0.8766 - val_accuracy: 0.2344\n",
      "Epoch 64/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3423 - accuracy: 0.4163 - val_loss: 0.8314 - val_accuracy: 0.2375\n",
      "Epoch 65/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.4183 - val_loss: 0.8039 - val_accuracy: 0.2391\n",
      "Epoch 66/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3415 - accuracy: 0.4173 - val_loss: 0.7962 - val_accuracy: 0.2438\n",
      "Epoch 67/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.4249 - val_loss: 0.8803 - val_accuracy: 0.2391\n",
      "Epoch 68/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3389 - accuracy: 0.4219 - val_loss: 0.9009 - val_accuracy: 0.2438\n",
      "Epoch 69/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3379 - accuracy: 0.4263 - val_loss: 0.9210 - val_accuracy: 0.2453\n",
      "Epoch 70/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3368 - accuracy: 0.4297 - val_loss: 0.9153 - val_accuracy: 0.2422\n",
      "Epoch 71/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3377 - accuracy: 0.4155 - val_loss: 0.9440 - val_accuracy: 0.2375\n",
      "Epoch 72/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.4212 - val_loss: 0.9520 - val_accuracy: 0.2484\n",
      "Epoch 73/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.4249 - val_loss: 1.0227 - val_accuracy: 0.2344\n",
      "Epoch 74/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.4253 - val_loss: 0.9242 - val_accuracy: 0.2414\n",
      "Epoch 75/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3377 - accuracy: 0.4215 - val_loss: 0.9771 - val_accuracy: 0.2414\n",
      "Epoch 76/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.4253 - val_loss: 0.9037 - val_accuracy: 0.2399\n",
      "Epoch 77/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3372 - accuracy: 0.4261 - val_loss: 0.9133 - val_accuracy: 0.2375\n",
      "Epoch 78/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.4227 - val_loss: 1.0141 - val_accuracy: 0.2422\n",
      "Epoch 79/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3355 - accuracy: 0.4235 - val_loss: 1.0141 - val_accuracy: 0.2375\n",
      "Epoch 80/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.4259 - val_loss: 0.9921 - val_accuracy: 0.2523\n",
      "Epoch 81/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3351 - accuracy: 0.4229 - val_loss: 0.9811 - val_accuracy: 0.2399\n",
      "Epoch 82/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.4259 - val_loss: 0.9081 - val_accuracy: 0.2391\n",
      "Epoch 83/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.4265 - val_loss: 0.9787 - val_accuracy: 0.2368\n",
      "Epoch 84/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.4221 - val_loss: 1.0411 - val_accuracy: 0.2430\n",
      "Epoch 85/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.4258 - val_loss: 1.0262 - val_accuracy: 0.2414\n",
      "Epoch 86/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.4186 - val_loss: 1.1005 - val_accuracy: 0.2336\n",
      "Epoch 87/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.4286 - val_loss: 1.0080 - val_accuracy: 0.2407\n",
      "Epoch 88/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.4237 - val_loss: 1.0389 - val_accuracy: 0.2430\n",
      "Epoch 89/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.4247 - val_loss: 1.0163 - val_accuracy: 0.2360\n",
      "Epoch 90/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3393 - accuracy: 0.4262 - val_loss: 0.9114 - val_accuracy: 0.2352\n",
      "Epoch 91/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3368 - accuracy: 0.4224 - val_loss: 0.9959 - val_accuracy: 0.2336\n",
      "Epoch 92/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3351 - accuracy: 0.4241 - val_loss: 1.0419 - val_accuracy: 0.2313\n",
      "Epoch 93/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.4226 - val_loss: 1.0493 - val_accuracy: 0.2251\n",
      "Epoch 94/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3338 - accuracy: 0.4257 - val_loss: 1.0509 - val_accuracy: 0.2336\n",
      "Epoch 95/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.4303 - val_loss: 1.0419 - val_accuracy: 0.2368\n",
      "Epoch 96/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3322 - accuracy: 0.4308 - val_loss: 1.1478 - val_accuracy: 0.2414\n",
      "Epoch 97/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3324 - accuracy: 0.4271 - val_loss: 1.1451 - val_accuracy: 0.2391\n",
      "Epoch 98/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.4266 - val_loss: 1.1409 - val_accuracy: 0.2298\n",
      "Epoch 99/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.4267 - val_loss: 1.2265 - val_accuracy: 0.2305\n",
      "Epoch 100/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.4335 - val_loss: 1.2594 - val_accuracy: 0.2445\n",
      "Epoch 101/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.4297 - val_loss: 1.2206 - val_accuracy: 0.2422\n",
      "Epoch 102/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3315 - accuracy: 0.4295 - val_loss: 1.2285 - val_accuracy: 0.2298\n",
      "Epoch 103/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.4264 - val_loss: 1.2276 - val_accuracy: 0.2329\n",
      "Epoch 104/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.4294 - val_loss: 1.1708 - val_accuracy: 0.2360\n",
      "Epoch 105/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.4245 - val_loss: 1.1715 - val_accuracy: 0.2360\n",
      "Epoch 106/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3322 - accuracy: 0.4312 - val_loss: 1.2467 - val_accuracy: 0.2344\n",
      "Epoch 107/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.4279 - val_loss: 1.2029 - val_accuracy: 0.2360\n",
      "Epoch 108/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3314 - accuracy: 0.4333 - val_loss: 1.1451 - val_accuracy: 0.2368\n",
      "Epoch 109/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.4305 - val_loss: 1.0697 - val_accuracy: 0.2360\n",
      "Epoch 110/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.4267 - val_loss: 0.9905 - val_accuracy: 0.2235\n",
      "Epoch 111/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.4276 - val_loss: 1.1008 - val_accuracy: 0.2422\n",
      "Epoch 112/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.4278 - val_loss: 0.9478 - val_accuracy: 0.2399\n",
      "Epoch 113/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.4298 - val_loss: 1.0674 - val_accuracy: 0.2360\n",
      "Epoch 114/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.4292 - val_loss: 1.0245 - val_accuracy: 0.2391\n",
      "Epoch 115/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.4297 - val_loss: 1.0889 - val_accuracy: 0.2375\n",
      "Epoch 116/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3302 - accuracy: 0.4348 - val_loss: 1.1268 - val_accuracy: 0.2414\n",
      "Epoch 117/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3311 - accuracy: 0.4275 - val_loss: 1.0973 - val_accuracy: 0.2391\n",
      "Epoch 118/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3305 - accuracy: 0.4273 - val_loss: 1.1488 - val_accuracy: 0.2407\n",
      "Epoch 119/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.4299 - val_loss: 1.1628 - val_accuracy: 0.2422\n",
      "Epoch 120/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.4306 - val_loss: 1.1386 - val_accuracy: 0.2430\n",
      "Epoch 121/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3322 - accuracy: 0.4229 - val_loss: 1.1311 - val_accuracy: 0.2298\n",
      "Epoch 122/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3291 - accuracy: 0.4253 - val_loss: 1.2251 - val_accuracy: 0.2430\n",
      "Epoch 123/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.4243 - val_loss: 1.1924 - val_accuracy: 0.2383\n",
      "Epoch 124/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3302 - accuracy: 0.4283 - val_loss: 1.2203 - val_accuracy: 0.2360\n",
      "Epoch 125/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.4292 - val_loss: 1.2745 - val_accuracy: 0.2407\n",
      "Epoch 126/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.4306 - val_loss: 1.2889 - val_accuracy: 0.2391\n",
      "Epoch 127/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.4316 - val_loss: 1.2861 - val_accuracy: 0.2321\n",
      "Epoch 128/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.4343 - val_loss: 1.4465 - val_accuracy: 0.2368\n",
      "Epoch 129/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.4314 - val_loss: 1.4027 - val_accuracy: 0.2422\n",
      "Epoch 130/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3286 - accuracy: 0.4313 - val_loss: 1.3199 - val_accuracy: 0.2360\n",
      "Epoch 131/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.4307 - val_loss: 1.2398 - val_accuracy: 0.2298\n",
      "Epoch 132/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.4279 - val_loss: 1.2120 - val_accuracy: 0.2259\n",
      "Epoch 133/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3281 - accuracy: 0.4287 - val_loss: 1.2262 - val_accuracy: 0.2282\n",
      "Epoch 134/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.4305 - val_loss: 1.3029 - val_accuracy: 0.2282\n",
      "Epoch 135/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3285 - accuracy: 0.4319 - val_loss: 1.3957 - val_accuracy: 0.2274\n",
      "Epoch 136/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.4248 - val_loss: 1.3357 - val_accuracy: 0.2375\n",
      "Epoch 137/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.4280 - val_loss: 1.0847 - val_accuracy: 0.2375\n",
      "Epoch 138/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.4319 - val_loss: 1.0866 - val_accuracy: 0.2336\n",
      "Epoch 139/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.4346 - val_loss: 1.1933 - val_accuracy: 0.2391\n",
      "Epoch 140/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.4323 - val_loss: 1.2315 - val_accuracy: 0.2407\n",
      "Epoch 141/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.4271 - val_loss: 1.2227 - val_accuracy: 0.2259\n",
      "Epoch 142/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.4312 - val_loss: 1.2086 - val_accuracy: 0.2352\n",
      "Epoch 143/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.4284 - val_loss: 1.2665 - val_accuracy: 0.2329\n",
      "Epoch 144/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3275 - accuracy: 0.4342 - val_loss: 1.2872 - val_accuracy: 0.2414\n",
      "Epoch 145/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.4351 - val_loss: 1.0564 - val_accuracy: 0.2336\n",
      "Epoch 146/150\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.4342 - val_loss: 1.0596 - val_accuracy: 0.2290\n",
      "Epoch 147/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.4341 - val_loss: 1.1719 - val_accuracy: 0.2321\n",
      "Epoch 148/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.4320 - val_loss: 1.2103 - val_accuracy: 0.2329\n",
      "Epoch 149/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.4326 - val_loss: 1.1681 - val_accuracy: 0.2399\n",
      "Epoch 150/150\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.4332 - val_loss: 1.3519 - val_accuracy: 0.2321\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1ffaafb2f40>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Jaccard model NO L2 NORM DEFAULT LR\n",
    "dense_model_jaccard.fit(X_train_meta_jaccard, Y_train, epochs=150, batch_size=128, validation_data=(X_val_meta_jaccard, Y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BiLSTM-GRU Model for statement features\n",
    "def create_bilstm_gru_model(vocab_size, embedding_dim, embedding_matrix, input_length):\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=input_length, trainable=False)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(50, return_sequences=True))(embedding)\n",
    "    bigru = Bidirectional(GRU(50, return_sequences=True, return_state=True))[0](embedding)\n",
    "    max_pool_lstm = GlobalMaxPooling1D()(bilstm)\n",
    "    max_pool_gru = GlobalMaxPooling1D()(bigru)\n",
    "    avg_pool_lstm = GlobalAveragePooling1D()(bilstm)\n",
    "    avg_pool_gru = GlobalAveragePooling1D()(bigru)\n",
    "    concat = Concatenate()([max_pool_lstm, max_pool_gru, avg_pool_lstm, avg_pool_gru])\n",
    "    output = Dense(1, activation='sigmoid')(concat)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create the Dense model\n",
    "dense_input_shape = X_train_meta.shape[1]\n",
    "dense_model = create_dense_model(dense_input_shape)\n",
    "\n",
    "# Create the BiLSTM-GRU model\n",
    "vocab_size = len(vocabulary_dict_custom) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = embedding_matrix_custom_100d\n",
    "input_length = 30\n",
    "bilstm_gru_model = create_bilstm_gru_model(vocab_size, embedding_dim, embedding_matrix, input_length)\n",
    "\n",
    "# Train the models\n",
    "dense_model.fit(X_train_meta, y_train, epochs=150, batch_size=128, validation_data=(X_val_meta, y_val))\n",
    "bilstm_gru_model.fit(X_train_custom, y_train, epochs=10, batch_size=64, validation_data=(X_val_custom, y_val))\n",
    "\n",
    "# Obtain predictions\n",
    "dense_preds = dense_model.predict(X_test_meta)\n",
    "bilstm_gru_preds = bilstm_gru_model.predict(X_test_custom)\n",
    "\n",
    "# Ensemble voting\n",
    "val_accuracies = [dense_model.evaluate(X_val_meta,\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_multi_input_model(input_shape_meta, vocab_size_pos, vocab_size_dep, num_classes, learning_rate):\n",
    "    # Define separate input layers for each feature type\n",
    "    input_meta = Input(shape=input_shape_meta, name=\"meta_input\")\n",
    "    input_pos = Input(shape=(None,), name=\"pos_input\")\n",
    "    input_dep = Input(shape=(None,), name=\"dep_input\")\n",
    "\n",
    "    # Define initial processing layers for each feature type\n",
    "    meta_dense = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(input_meta)\n",
    "    pos_embedding = Embedding(vocab_size_pos, 16)(input_pos)\n",
    "    pos_flat = Flatten()(pos_embedding)\n",
    "    dep_embedding = Embedding(vocab_size_dep, 16)(input_dep)\n",
    "    dep_flat = Flatten()(dep_embedding)\n",
    "\n",
    "    # Concatenate the outputs of the initial processing layers\n",
    "    concat = Concatenate()([meta_dense, pos_flat, dep_flat])\n",
    "\n",
    "    # Add additional Dense layers after concatenation\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(concat)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "\n",
    "    # Add the final output layer\n",
    "    output = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))(dense)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[input_meta, input_pos, input_dep], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
